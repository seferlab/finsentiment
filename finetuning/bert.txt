-- first save a tokenizer model using bert_tokenizer.ipynb file
-- then, download transformers 
	git clone https://github.com/huggingface/transformers.git
	also,
	pip install datasets

-- then, go to /transformers/examples/pytorch/language-modeling/run_mlm.py file
-- change around line 293
	    if model_args.tokenizer_name:
        	#tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)
        	from transformers import BertTokenizerFast, RobertaTokenizerFast
        	#for bert uncomment
        	tokenizer = BertTokenizerFast.from_pretrained("./path_to_bert_tokenizer", max_len=512)
        	#for roberta uncomment
        	#tokenizer = RobertaTokenizerFast.from_pretrained("./path_to_roberta_tokenizer", max_len=512)

-- then,


nohup python -u /path_to_transformers_file/transformers/transformers/examples/pytorch/language-modeling/run_mlm.py --train_file "/path_to_textfile_to_be_finetuned.txt" --tokenizer_name "not_important" --do_train --max_steps 1000 --model_type bert --pad_to_max_length True --output_dir ./output_bert