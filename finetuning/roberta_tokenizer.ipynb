{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"roberta_tokenizer.ipynb","provenance":[{"file_id":"1MoN1mBR-mowcgCnpXXIa1T_0AEB0xlhx","timestamp":1622553118254}],"collapsed_sections":[],"authorship_tag":"ABX9TyM0AovIPJSxLGzN6DLtQFDY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"fcVxmj4op3xn"},"source":["# We won't need TensorFlow here\n","!pip uninstall -y tensorflow\n","# Install `transformers` from master\n","!pip install git+https://github.com/huggingface/transformers\n","!pip list | grep -E 'transformers|tokenizers'\n","# transformers version at notebook update --- 2.11.0\n","# tokenizers version at notebook update --- 0.8.0rc1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i9SzrLbip78Q"},"source":["%%time \n","from pathlib import Path\n","\n","from tokenizers import ByteLevelBPETokenizer\n","paths = r\"/content/drive/MyDrive/100mbout/out2006_3.txt\"\n","# Initialize a tokenizer\n","tokenizer = BertWordPieceTokenizer()\n","\n","# Customize training\n","tokenizer = ByteLevelBPETokenizer()\n","\n","# Customize training\n","tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n","    \"<s>\",\n","    \"<pad>\",\n","    \"</s>\",\n","    \"<unk>\",\n","    \"<mask>\",\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W3vjMoLAqD66"},"source":["!mkdir robertam\n","tokenizer.save_model(\"robertam\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nlJvH918szsz"},"source":["from tokenizers.implementations import ByteLevelBPETokenizer\n","from tokenizers.processors import BertProcessing\n","\n","\n","tokenizer = ByteLevelBPETokenizer(\n","    \"./robertam/vocab.json\",\n","    \"./robertam/merges.txt\",\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mrx7IMXGs0g_"},"source":["tokenizer._tokenizer.post_processor = BertProcessing(\n","    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n","    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",")\n","tokenizer.enable_truncation(max_length=512)"],"execution_count":null,"outputs":[]}]}