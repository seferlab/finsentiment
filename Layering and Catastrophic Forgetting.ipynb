{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Layering and Catastrophic Forgetting.ipynb","provenance":[{"file_id":"18nNk3i-sOzIiRctaWTfqQkef4f5emENo","timestamp":1647280315248},{"file_id":"1kQyWixHpUxfNHiUZe53AtgySST1T5RxH","timestamp":1635959118243},{"file_id":"1umTv4CUigpj8c7ISVBRgMss51_IbmRaq","timestamp":1635494009940},{"file_id":"1nd9Z0_Y_j-Q0RTl05LFbe1ij5eJ_Tpl5","timestamp":1635440938756},{"file_id":"1Ey2KOOT7mcftuaPH-tpI-fk_Le_vRd5r","timestamp":1632739847247},{"file_id":"1qYOOHqnDhK9UQyWXs3nHZqocY_bFSrqk","timestamp":1632494601044}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**ref: https://simpletransformers.ai/**\n","ClassificationModel from simpletransformers is manipulated!"],"metadata":{"id":"q1vQ-hFhjEGP"}},{"cell_type":"code","metadata":{"id":"keqrDAp1dRB4"},"source":["!pip install transformers\n","!pip install torch\n","!pip install scikit-learn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wbsTiPXGGpTc","executionInfo":{"status":"ok","timestamp":1636793628946,"user_tz":-180,"elapsed":18622,"user":{"displayName":"z erva erg√ºn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16255015835614953460"}},"outputId":"362a2db6-9d87-43ba-eb4f-99e5a3653ab9"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"n4U25qPKemlm"},"source":["!pip install simpletransformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b_6jMDRUdX2l"},"source":["import transformers\n","from transformers import XLNetTokenizer, XLNetModel, AdamW, get_linear_schedule_with_warmup\n","import torch\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib import rc\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n","from collections import defaultdict\n","from textwrap import wrap\n","from pylab import rcParams\n","\n","from torch import nn, optim\n","from keras.preprocessing.sequence import pad_sequences\n","from torch.utils.data import TensorDataset,RandomSampler,SequentialSampler\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4CDkyu_ndyR0"},"source":["import pandas as pd\n","train = pd.read_csv('/content/drive/MyDrive/100mbout/arrangedallagree.csv' ,encoding=\"latin\",header=None)#, names=['No','text','labels'])\n","train.columns = [\"text\",'labels']\n","val = pd.read_csv('/content/drive/MyDrive/100mbout/arranged75agree.csv' ,encoding=\"latin\",header=None)#, names=['No','text','labels'])\n","val.columns = [\"text\",'labels']\n","from sklearn.model_selection import train_test_split\n","label_list = [-1,0,1] # 0,1,2\n","train.labels+=1\n","val.labels+=1\n","train = train[['text','labels']]\n","val = val[['text','labels']]\n","\n","CUDA_LAUNCH_BLOCKING=1 \n","label_list = [-1,0,1] # 0,1,2\n","\n","train = train[['text','labels']]\n","\n","train.head()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eufi7LbdUeCD"},"source":["from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n","\n","\n","def f1_multiclass(labels, preds):\n","    return f1_score(labels, preds, average='micro')\n","def f1_multiclass2(labels, preds):\n","    return f1_score(labels, preds, average='macro')\n","def f1_multiclass3(labels, preds):\n","    return f1_score(labels, preds, average='weighted')\n","def prec_score(labels, preds):\n","    return precision_score(labels, preds, average='micro')\n","def prec_score2(labels, preds):\n","    return precision_score(labels, preds, average='macro')\n","def prec_score3(labels, preds):\n","    return precision_score(labels, preds, average='weighted')\n","def recall(labels, preds):\n","    return recall_score(labels, preds, average='micro')\n","def recall2(labels, preds):\n","    return recall_score(labels, preds, average='macro')\n","def recall3(labels, preds):\n","    return recall_score(labels, preds, average='weighted')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# STLR"],"metadata":{"id":"MovUFrskkF5f"}},{"cell_type":"markdown","source":["STLR scheduler implementation:"],"metadata":{"id":"gDmhO9cqkBoS"}},{"cell_type":"code","metadata":{"id":"mu0RmjSgDj_X"},"source":["class STLR(torch.optim.lr_scheduler._LRScheduler):\n","    def __init__(self, optimizer, max_mul, ratio, steps_per_cycle, decay=1, last_epoch=-1):\n","        self.max_mul = max_mul - 1\n","        self.turning_point = steps_per_cycle // (ratio + 1)\n","        self.steps_per_cycle = steps_per_cycle\n","        self.decay = decay\n","        super().__init__(optimizer, last_epoch)\n","\n","    def get_lr(self):\n","        residual = self.last_epoch % self.steps_per_cycle\n","        multiplier = self.decay ** (self.last_epoch // self.steps_per_cycle)\n","        if residual <= self.turning_point:\n","            multiplier *= self.max_mul * (residual / self.turning_point)\n","        else:\n","            multiplier *= self.max_mul * (\n","                (self.steps_per_cycle - residual) /\n","                (self.steps_per_cycle - self.turning_point))\n","        return [lr * (1 + multiplier) for lr in self.base_lrs]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d0iE7IDU8rAM"},"source":["# Implementation of dft and gu to Classification Model from Simpletransformers"]},{"cell_type":"markdown","source":["Also layering variable is gets the to-be -observed layering number"],"metadata":{"id":"vI2nJfTelurk"}},{"cell_type":"code","metadata":{"id":"QKM1Dw2qC7yb"},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Fri Oct 29 19:55:30 2021\n","\n","@author: Erva\n","\"\"\"\n","\n","from __future__ import absolute_import, division, print_function\n","import collections\n","import logging\n","import math\n","import os\n","import random\n","import warnings\n","from dataclasses import asdict\n","from multiprocessing import cpu_count\n","import tempfile\n","from pathlib import Path\n","\n","from collections import Counter\n","import numpy as np\n","import pandas as pd\n","import torch\n","from scipy.stats import mode, pearsonr\n","from scipy.special import softmax\n","from sklearn.metrics import (\n","    confusion_matrix,\n","    label_ranking_average_precision_score,\n","    matthews_corrcoef,\n","    mean_squared_error,\n","    roc_curve,\n","    auc,\n","    average_precision_score,\n",")\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.nn import CrossEntropyLoss\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm.auto import tqdm, trange\n","from tqdm.contrib import tenumerate\n","from transformers.optimization import (\n","    get_constant_schedule,\n","    get_constant_schedule_with_warmup,\n","    get_linear_schedule_with_warmup,\n","    get_cosine_schedule_with_warmup,\n","    get_cosine_with_hard_restarts_schedule_with_warmup,\n","    get_polynomial_decay_schedule_with_warmup,\n",")\n","from transformers.optimization import AdamW, Adafactor\n","from transformers import (\n","    AlbertConfig,\n","    AlbertTokenizer,\n","    AlbertForSequenceClassification,\n","    AutoConfig,\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    BertConfig,\n","    BertTokenizerFast,\n","    BertForSequenceClassification,\n","    BertweetTokenizer,\n","    BigBirdConfig,\n","    BigBirdTokenizer,\n","    BigBirdForSequenceClassification,\n","    CamembertConfig,\n","    CamembertTokenizerFast,\n","    CamembertForSequenceClassification,\n","    DebertaConfig,\n","    DebertaForSequenceClassification,\n","    DebertaTokenizer,\n","    DebertaV2Config,\n","    DebertaV2ForSequenceClassification,\n","    DebertaV2Tokenizer,\n","    DistilBertConfig,\n","    DistilBertTokenizerFast,\n","    DistilBertForSequenceClassification,\n","    ElectraConfig,\n","    ElectraTokenizerFast,\n","    ElectraForSequenceClassification,\n","    FlaubertConfig,\n","    FlaubertTokenizer,\n","    FlaubertForSequenceClassification,\n","    HerbertTokenizerFast,\n","    LayoutLMConfig,\n","    LayoutLMTokenizerFast,\n","    LayoutLMForSequenceClassification,\n","    LongformerConfig,\n","    LongformerTokenizerFast,\n","    LongformerForSequenceClassification,\n","    MPNetConfig,\n","    MPNetForSequenceClassification,\n","    MPNetTokenizerFast,\n","    MobileBertConfig,\n","    MobileBertTokenizerFast,\n","    MobileBertForSequenceClassification,\n","    RobertaConfig,\n","    RobertaTokenizerFast,\n","    RobertaForSequenceClassification,\n","    SqueezeBertConfig,\n","    SqueezeBertForSequenceClassification,\n","    SqueezeBertTokenizerFast,\n","    WEIGHTS_NAME,\n","    XLMConfig,\n","    XLMRobertaConfig,\n","    XLMRobertaTokenizerFast,\n","    XLMRobertaForSequenceClassification,\n","    XLMTokenizer,\n","    XLMForSequenceClassification,\n","    XLNetConfig,\n","    XLNetTokenizerFast,\n","    XLNetForSequenceClassification,\n",")\n","from transformers.convert_graph_to_onnx import convert, quantize\n","\n","from simpletransformers.classification.classification_utils import (\n","    InputExample,\n","    LazyClassificationDataset,\n","    ClassificationDataset,\n","    convert_examples_to_features,\n","    load_hf_dataset,\n","    flatten_results,\n",")\n","from simpletransformers.config.global_args import global_args\n","from simpletransformers.config.model_args import ClassificationArgs\n","from simpletransformers.config.utils import sweep_config_to_sweep_values\n","from simpletransformers.losses.loss_utils import init_loss\n","\n","# from simpletransformers.custom_models.models import ElectraForSequenceClassification\n","\n","\n","try:\n","    import wandb\n","\n","    wandb_available = True\n","except ImportError:\n","    wandb_available = False\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","MODELS_WITHOUT_CLASS_WEIGHTS_SUPPORT = [\"squeezebert\", \"deberta\", \"mpnet\"]\n","\n","MODELS_WITH_EXTRA_SEP_TOKEN = [\n","    \"roberta\",\n","    \"camembert\",\n","    \"xlmroberta\",\n","    \"longformer\",\n","    \"mpnet\",\n","]\n","\n","MODELS_WITH_ADD_PREFIX_SPACE = [\n","    \"roberta\",\n","    \"camembert\",\n","    \"xlmroberta\",\n","    \"longformer\",\n","    \"mpnet\",\n","]\n","\n","MODELS_WITHOUT_SLIDING_WINDOW_SUPPORT = [\"squeezebert\"]\n","\n","\n","class ClassificationModelZ:\n","    def __init__(\n","        self,\n","        model_type,\n","        model_name,\n","        layering,\n","        tokenizer_type=None,\n","        tokenizer_name=None,\n","        num_labels=None,\n","        weight=None,\n","        args=None,\n","        use_cuda=True,\n","        cuda_device=-1,\n","        onnx_execution_provider=None,\n","        **kwargs,\n","    ):\n","\n","        \"\"\"\n","        Initializes a ClassificationModel model.\n","        Args:\n","            model_type: The type of model (bert, xlnet, xlm, roberta, distilbert)\n","            model_name: The exact architecture and trained weights to use. This may be a Hugging Face Transformers compatible pre-trained model, a community model, or the path to a directory containing model files.\n","            tokenizer_type: The type of tokenizer (auto, bert, xlnet, xlm, roberta, distilbert, etc.) to use. If a string is passed, Simple Transformers will try to initialize a tokenizer class from the available MODEL_CLASSES.\n","                                Alternatively, a Tokenizer class (subclassed from PreTrainedTokenizer) can be passed.\n","            tokenizer_name: The name/path to the tokenizer. If the tokenizer_type is not specified, the model_type will be used to determine the type of the tokenizer.\n","            num_labels (optional): The number of labels or classes in the dataset.\n","            weight (optional): A list of length num_labels containing the weights to assign to each label for loss calculation.\n","            args (optional): Default args will be used if this parameter is not provided. If provided, it should be a dict containing the args that should be changed in the default args.\n","            use_cuda (optional): Use GPU if available. Setting to False will force model to use CPU only.\n","            cuda_device (optional): Specific GPU that should be used. Will use the first available GPU by default.\n","            onnx_execution_provider (optional): ExecutionProvider to use with ONNX Runtime. Will use CUDA (if use_cuda) or CPU (if use_cuda is False) by default.\n","            **kwargs (optional): For providing proxies, force_download, resume_download, cache_dir and other options specific to the 'from_pretrained' implementation where this will be supplied.\n","        \"\"\"  # noqa: ignore flake8\"\n","\n","        MODEL_CLASSES = {\n","            \"albert\": (AlbertConfig, AlbertForSequenceClassification, AlbertTokenizer),\n","            \"auto\": (AutoConfig, AutoModelForSequenceClassification, AutoTokenizer),\n","            \"bert\": (BertConfig, BertForSequenceClassification, BertTokenizerFast),\n","            \"bertweet\": (\n","                RobertaConfig,\n","                RobertaForSequenceClassification,\n","                BertweetTokenizer,\n","            ),\n","            \"bigbird\": (\n","                BigBirdConfig,\n","                BigBirdForSequenceClassification,\n","                BigBirdTokenizer,\n","            ),\n","            \"camembert\": (\n","                CamembertConfig,\n","                CamembertForSequenceClassification,\n","                CamembertTokenizerFast,\n","            ),\n","            \"deberta\": (\n","                DebertaConfig,\n","                DebertaForSequenceClassification,\n","                DebertaTokenizer,\n","            ),\n","            \"debertav2\": (\n","                DebertaV2Config,\n","                DebertaV2ForSequenceClassification,\n","                DebertaV2Tokenizer,\n","            ),\n","            \"distilbert\": (\n","                DistilBertConfig,\n","                DistilBertForSequenceClassification,\n","                DistilBertTokenizerFast,\n","            ),\n","            \"electra\": (\n","                ElectraConfig,\n","                ElectraForSequenceClassification,\n","                ElectraTokenizerFast,\n","            ),\n","            \"flaubert\": (\n","                FlaubertConfig,\n","                FlaubertForSequenceClassification,\n","                FlaubertTokenizer,\n","            ),\n","            \"herbert\": (\n","                BertConfig,\n","                BertForSequenceClassification,\n","                HerbertTokenizerFast,\n","            ),\n","            \"layoutlm\": (\n","                LayoutLMConfig,\n","                LayoutLMForSequenceClassification,\n","                LayoutLMTokenizerFast,\n","            ),\n","            \"longformer\": (\n","                LongformerConfig,\n","                LongformerForSequenceClassification,\n","                LongformerTokenizerFast,\n","            ),\n","            \"mobilebert\": (\n","                MobileBertConfig,\n","                MobileBertForSequenceClassification,\n","                MobileBertTokenizerFast,\n","            ),\n","            \"mpnet\": (MPNetConfig, MPNetForSequenceClassification, MPNetTokenizerFast),\n","            \"roberta\": (\n","                RobertaConfig,\n","                RobertaForSequenceClassification,\n","                RobertaTokenizerFast,\n","            ),\n","            \"squeezebert\": (\n","                SqueezeBertConfig,\n","                SqueezeBertForSequenceClassification,\n","                SqueezeBertTokenizerFast,\n","            ),\n","            \"xlm\": (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n","            \"xlmroberta\": (\n","                XLMRobertaConfig,\n","                XLMRobertaForSequenceClassification,\n","                XLMRobertaTokenizerFast,\n","            ),\n","            \"xlnet\": (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizerFast),\n","        }\n","\n","        self.args = self._load_model_args(model_name)\n","\n","        if isinstance(args, dict):\n","            self.args.update_from_dict(args)\n","        elif isinstance(args, ClassificationArgs):\n","            self.args = args\n","\n","        if (\n","            model_type in MODELS_WITHOUT_SLIDING_WINDOW_SUPPORT\n","            and self.args.sliding_window\n","        ):\n","            raise ValueError(\n","                \"{} does not currently support sliding window\".format(model_type)\n","            )\n","        if layering<=11 or layering>=0:\n","            will = True\n","        if self.args.thread_count:\n","            torch.set_num_threads(self.args.thread_count)\n","\n","        if \"sweep_config\" in kwargs:\n","            self.is_sweeping = True\n","            sweep_config = kwargs.pop(\"sweep_config\")\n","            sweep_values = sweep_config_to_sweep_values(sweep_config)\n","            self.args.update_from_dict(sweep_values)\n","        else:\n","            self.is_sweeping = False\n","\n","        if self.args.manual_seed:\n","            random.seed(self.args.manual_seed)\n","            np.random.seed(self.args.manual_seed)\n","            torch.manual_seed(self.args.manual_seed)\n","            if self.args.n_gpu > 0:\n","                torch.cuda.manual_seed_all(self.args.manual_seed)\n","\n","        if self.args.labels_list:\n","            if num_labels:\n","                assert num_labels == len(self.args.labels_list)\n","            if self.args.labels_map:\n","                try:\n","                    assert list(self.args.labels_map.keys()) == self.args.labels_list\n","                except AssertionError:\n","                    assert [\n","                        int(key) for key in list(self.args.labels_map.keys())\n","                    ] == self.args.labels_list\n","                    self.args.labels_map = {\n","                        int(key): value for key, value in self.args.labels_map.items()\n","                    }\n","            else:\n","                self.args.labels_map = {\n","                    label: i for i, label in enumerate(self.args.labels_list)\n","                }\n","        else:\n","            len_labels_list = 2 if not num_labels else num_labels\n","            self.args.labels_list = [i for i in range(len_labels_list)]\n","\n","        config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n","\n","        if tokenizer_type is not None:\n","            if isinstance(tokenizer_type, str):\n","                _, _, tokenizer_class = MODEL_CLASSES[tokenizer_type]\n","            else:\n","                tokenizer_class = tokenizer_type\n","\n","        if num_labels:\n","            self.config = config_class.from_pretrained(\n","                model_name, num_labels=num_labels, **self.args.config\n","            )\n","            self.num_labels = num_labels\n","        else:\n","            self.config = config_class.from_pretrained(model_name, **self.args.config)\n","            self.num_labels = self.config.num_labels\n","\n","        if model_type in MODELS_WITHOUT_CLASS_WEIGHTS_SUPPORT and weight is not None:\n","            raise ValueError(\n","                \"{} does not currently support class weights\".format(model_type)\n","            )\n","        else:\n","            self.weight = weight\n","\n","        if use_cuda:\n","            if torch.cuda.is_available():\n","                if cuda_device == -1:\n","                    self.device = torch.device(\"cuda\")\n","                else:\n","                    self.device = torch.device(f\"cuda:{cuda_device}\")\n","            else:\n","                raise ValueError(\n","                    \"'use_cuda' set to True when cuda is unavailable.\"\n","                    \" Make sure CUDA is available or set use_cuda=False.\"\n","                )\n","        else:\n","            self.device = \"cpu\"\n","\n","        self.loss_fct = init_loss(\n","            weight=self.weight, device=self.device, args=self.args\n","        )\n","\n","        if self.args.onnx:\n","            from onnxruntime import InferenceSession, SessionOptions\n","\n","            if not onnx_execution_provider:\n","                onnx_execution_provider = (\n","                    \"CUDAExecutionProvider\" if use_cuda else \"CPUExecutionProvider\"\n","                )\n","\n","            options = SessionOptions()\n","\n","            if self.args.dynamic_quantize:\n","                model_path = quantize(Path(os.path.join(model_name, \"onnx_model.onnx\")))\n","                self.model = InferenceSession(\n","                    model_path.as_posix(), options, providers=[onnx_execution_provider]\n","                )\n","            else:\n","                model_path = os.path.join(model_name, \"onnx_model.onnx\")\n","                self.model = InferenceSession(\n","                    model_path, options, providers=[onnx_execution_provider]\n","                )\n","        else:\n","            if not self.args.quantized_model:\n","                self.model = model_class.from_pretrained(\n","                    model_name, config=self.config, **kwargs\n","                )\n","            else:\n","                quantized_weights = torch.load(\n","                    os.path.join(model_name, \"pytorch_model.bin\")\n","                )\n","\n","                self.model = model_class.from_pretrained(\n","                    None, config=self.config, state_dict=quantized_weights\n","                )\n","\n","            if self.args.dynamic_quantize:\n","                self.model = torch.quantization.quantize_dynamic(\n","                    self.model, {torch.nn.Linear}, dtype=torch.qint8\n","                )\n","            if self.args.quantized_model:\n","                self.model.load_state_dict(quantized_weights)\n","            if self.args.dynamic_quantize:\n","                self.args.quantized_model = True\n","\n","        self.results = {}\n","\n","        if not use_cuda:\n","            self.args.fp16 = False\n","\n","        if self.args.fp16:\n","            try:\n","                from torch.cuda import amp\n","            except AttributeError:\n","                raise AttributeError(\n","                    \"fp16 requires Pytorch >= 1.6. Please update Pytorch or turn off fp16.\"\n","                )\n","\n","        if tokenizer_name is None:\n","            tokenizer_name = model_name\n","\n","        if tokenizer_name in [\n","            \"vinai/bertweet-base\",\n","            \"vinai/bertweet-covid19-base-cased\",\n","            \"vinai/bertweet-covid19-base-uncased\",\n","        ]:\n","            self.tokenizer = tokenizer_class.from_pretrained(\n","                tokenizer_name,\n","                do_lower_case=self.args.do_lower_case,\n","                normalization=True,\n","                **kwargs,\n","            )\n","        else:\n","            self.tokenizer = tokenizer_class.from_pretrained(\n","                tokenizer_name, do_lower_case=self.args.do_lower_case, **kwargs\n","            )\n","\n","        if self.args.special_tokens_list:\n","            self.tokenizer.add_tokens(\n","                self.args.special_tokens_list, special_tokens=True\n","            )\n","            self.model.resize_token_embeddings(len(self.tokenizer))\n","\n","        self.args.model_name = model_name\n","        self.args.model_type = model_type\n","        self.args.tokenizer_name = tokenizer_name\n","        self.args.tokenizer_type = tokenizer_type\n","        self.args.layering = layering\n","        if model_type in [\"camembert\", \"xlmroberta\"]:\n","            warnings.warn(\n","                f\"use_multiprocessing automatically disabled as {model_type}\"\n","                \" fails when using multiprocessing for feature conversion.\"\n","            )\n","            self.args.use_multiprocessing = False\n","\n","        if self.args.wandb_project and not wandb_available:\n","            warnings.warn(\n","                \"wandb_project specified but wandb is not available. Wandb disabled.\"\n","            )\n","            self.args.wandb_project = None\n","\n","    def train_model(\n","        self,\n","        train_df,\n","        #another_df,\n","        multi_label=False,\n","        output_dir=None,\n","        show_running_loss=True,\n","        args=None,\n","        eval_df=None,\n","        verbose=True,\n","        **kwargs,\n","    ):\n","        \"\"\"\n","        Trains the model using 'train_df'\n","        Args:\n","            train_df: Pandas Dataframe containing at least two columns. If the Dataframe has a header, it should contain a 'text' and a 'labels' column. If no header is present,\n","            the Dataframe should contain at least two columns, with the first column containing the text, and the second column containing the label. The model will be trained on this Dataframe.\n","            output_dir: The directory where model files will be saved. If not given, self.args.output_dir will be used.\n","            show_running_loss (optional): Set to False to prevent running loss from being printed to console. Defaults to True.\n","            args (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.\n","            eval_df (optional): A DataFrame against which evaluation will be performed when evaluate_during_training is enabled. Is required if evaluate_during_training is enabled.\n","            **kwargs: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use). E.g. f1=sklearn.metrics.f1_score.\n","                        A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions.\n","        Returns:\n","            global_step: Number of global steps trained\n","            training_details: Average training loss if evaluate_during_training is False or full training progress scores if evaluate_during_training is True\n","        \"\"\"  # noqa: ignore flake8\"\n","        if args:\n","            self.args.update_from_dict(args)\n","\n","        if self.args.silent:\n","            show_running_loss = False\n","\n","        if self.args.evaluate_during_training and eval_df is None:\n","            raise ValueError(\n","                \"evaluate_during_training is enabled but eval_df is not specified.\"\n","                \" Pass eval_df to model.train_model() if using evaluate_during_training.\"\n","            )\n","\n","        if not output_dir:\n","            output_dir = self.args.output_dir\n","\n","        if (\n","            os.path.exists(output_dir)\n","            and os.listdir(output_dir)\n","            and not self.args.overwrite_output_dir\n","        ):\n","            raise ValueError(\n","                \"Output directory ({}) already exists and is not empty.\"\n","                \" Set overwrite_output_dir: True to automatically overwrite.\".format(\n","                    output_dir\n","                )\n","            )\n","        self._move_model_to_device()\n","\n","        if self.args.use_hf_datasets:\n","            if self.args.sliding_window:\n","                raise ValueError(\n","                    \"HuggingFace Datasets cannot be used with sliding window.\"\n","                )\n","            if self.args.model_type == \"layoutlm\":\n","                raise NotImplementedError(\n","                    \"HuggingFace Datasets support is not implemented for LayoutLM models\"\n","                )\n","            train_dataset = load_hf_dataset(\n","                train_df, self.tokenizer, self.args, multi_label=multi_label\n","            )\n","        elif isinstance(train_df, str) and self.args.lazy_loading:\n","            if self.args.sliding_window:\n","                raise ValueError(\"Lazy loading cannot be used with sliding window.\")\n","            if self.args.model_type == \"layoutlm\":\n","                raise NotImplementedError(\n","                    \"Lazy loading is not implemented for LayoutLM models\"\n","                )\n","            train_dataset = LazyClassificationDataset(\n","                train_df, self.tokenizer, self.args\n","            )\n","        else:\n","            if self.args.lazy_loading:\n","                raise ValueError(\n","                    \"Input must be given as a path to a file when using lazy loading\"\n","                )\n","            if \"text\" in train_df.columns and \"labels\" in train_df.columns:\n","                if self.args.model_type == \"layoutlm\":\n","                    train_examples = [\n","                        InputExample(i, text, None, label, x0, y0, x1, y1)\n","                        for i, (text, label, x0, y0, x1, y1) in enumerate(\n","                            zip(\n","                                train_df[\"text\"].astype(str),\n","                                train_df[\"labels\"],\n","                                train_df[\"x0\"],\n","                                train_df[\"y0\"],\n","                                train_df[\"x1\"],\n","                                train_df[\"y1\"],\n","                            )\n","                        )\n","                    ]\n","                else:\n","                    train_examples = (\n","                        train_df[\"text\"].astype(str).tolist(),\n","                        train_df[\"labels\"].tolist(),\n","                    )\n","            elif \"text_a\" in train_df.columns and \"text_b\" in train_df.columns:\n","                if self.args.model_type == \"layoutlm\":\n","                    raise ValueError(\"LayoutLM cannot be used with sentence-pair tasks\")\n","                else:\n","                    train_examples = (\n","                        train_df[\"text_a\"].astype(str).tolist(),\n","                        train_df[\"text_b\"].astype(str).tolist(),\n","                        train_df[\"labels\"].tolist(),\n","                    )\n","            else:\n","                warnings.warn(\n","                    \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n","                )\n","                train_examples = (\n","                    train_df.iloc[:, 0].astype(str).tolist(),\n","                    train_df.iloc[:, 1].tolist(),\n","                )\n","            train_dataset = self.load_and_cache_examples(\n","                train_examples, verbose=verbose\n","            )\n","        train_sampler = RandomSampler(train_dataset)\n","        train_dataloader = DataLoader(\n","            train_dataset,\n","            sampler=train_sampler,\n","            batch_size=self.args.train_batch_size,\n","            num_workers=self.args.dataloader_num_workers,\n","        )\n","\n","        os.makedirs(output_dir, exist_ok=True)\n","        layer_num =0\n","        global_step, training_details = self.train(\n","            train_dataloader,\n","            train_df,\n","            output_dir,\n","            layer_num,\n","            multi_label=multi_label,\n","            show_running_loss=show_running_loss,\n","            eval_df=eval_df,\n","            verbose=verbose,\n","            **kwargs,\n","        )\n","\n","        # model_to_save = self.model.module if hasattr(self.model, \"module\") else self.model\n","        # model_to_save.save_pretrained(output_dir)\n","        # self.tokenizer.save_pretrained(output_dir)\n","        # torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\n","        self.save_model(model=self.model)\n","\n","        if verbose:\n","            logger.info(\n","                \" Training of {} model complete. Saved to {}.\".format(\n","                    self.args.model_type, output_dir\n","                )\n","            )\n","\n","        return global_step, training_details\n","    def roberta_base_AdamW_LLRD2(self,model,stype, debug=False):\n","        #model = model.roberta\n","        if self.args.model_type==\"roberta\":\n","          print(\"ebeelelele\")\n","          model = model.roberta\n","        if self.args.model_type=='bert':\n","          print(\"else!!!!!\")\n","          model =model.bert\n","        opt_parameters = [] # To be passed to the optimizer (only parameters of the layers you want to update).\n","        named_parameters = list(model.named_parameters()) \n","        debug_param_groups = []\n","        \n","        # According to AAAMLP book by A. Thakur, we generally do not use any decay \n","        # for bias and LayerNorm.weight layers.\n","        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","        init_lr = 1e-6\n","        head_lr = 1e-6\n","        lr = init_lr\n","        \n","        # =============================================================================\n","        params_0 = [p for n,p in named_parameters if (\"pooler\" in n or \"regressor\" in n) \n","                    and any(nd in n for nd in no_decay)]\n","        params_1 = [p for n,p in named_parameters if (\"pooler\" in n or \"regressor\" in n)\n","                    and not any(nd in n for nd in no_decay)]\n","        \n","        head_params = {\"params\": params_0, \"lr\": head_lr, \"weight_decay\": 0.0}    \n","        opt_parameters.append(head_params)\n","        debug_param_groups.append(f\"head_params\")\n","        \n","        head_params = {\"params\": params_1, \"lr\": head_lr, \"weight_decay\": 0.01}    \n","        opt_parameters.append(head_params)\n","        debug_param_groups.append(f\"head_params\")\n","                \n","        # =============================================================================        \n","        for layer in range(11,-1,-1):\n","            \n","            params_0 = [p for n,p in named_parameters if f\"encoder.layer.{layer}.\" in n \n","                        and any(nd in n for nd in no_decay)]\n","            params_1 = [p for n,p in named_parameters if f\"encoder.layer.{layer}.\" in n \n","                        and not any(nd in n for nd in no_decay)]\n","            \n","            layer_params = {\"params\": params_0, \"lr\": lr, \"weight_decay\": 0.0}\n","            opt_parameters.append(layer_params)   \n","            debug_param_groups.append(f\"layer.{layer}\")\n","                        \n","            layer_params = {\"params\": params_1, \"lr\": lr, \"weight_decay\": 0.01}\n","            opt_parameters.append(layer_params)\n","            debug_param_groups.append(f\"layer.{layer}\")       \n","            \n","            lr *= 1.001 \n","        \n","        # =============================================================================\n","        params_0 = [p for n,p in named_parameters if \"embeddings\" in n \n","                    and any(nd in n for nd in no_decay)]\n","        params_1 = [p for n,p in named_parameters if \"embeddings\" in n\n","                    and not any(nd in n for nd in no_decay)]\n","        \n","        embed_params = {\"params\": params_0, \"lr\": lr, \"weight_decay\": 0.0} \n","        opt_parameters.append(embed_params)\n","        debug_param_groups.append(f\"embed_params\")\n","        \n","        embed_params = {\"params\": params_1, \"lr\": lr, \"weight_decay\": 0.01} \n","        opt_parameters.append(embed_params)\n","        debug_param_groups.append(f\"embed_params\")\n","        \n","        if debug: \n","            for g in range(len(debug_param_groups)): print(g, debug_param_groups[g]) \n","        \n","        return transformers.AdamW(opt_parameters, lr=init_lr), debug_param_groups\n","    def train(\n","        self,\n","        train_dataloader,\n","        another_df,\n","        output_dir,\n","        layer_num,\n","        multi_label=False,\n","        show_running_loss=True,\n","        eval_df=None,\n","        test_df=None,\n","        verbose=True,\n","        **kwargs,\n","    ):\n","        \"\"\"\n","        Trains the model on train_dataset.\n","        Utility function to be used by the train_model() method. Not intended to be used directly.\n","        \"\"\"\n","        \n","        model = self.model\n","        args = self.args\n","\n","        tb_writer = SummaryWriter(log_dir=args.tensorboard_dir)\n","\n","        t_total = (\n","            len(train_dataloader)\n","            // args.gradient_accumulation_steps\n","            * args.num_train_epochs\n","        )\n","\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","\n","        optimizer_grouped_parameters = []\n","        custom_parameter_names = set()\n","        for group in self.args.custom_parameter_groups:\n","            params = group.pop(\"params\")\n","            custom_parameter_names.update(params)\n","            param_group = {**group}\n","            param_group[\"params\"] = [\n","                p for n, p in model.named_parameters() if n in params\n","            ]\n","            optimizer_grouped_parameters.append(param_group)\n","\n","        for group in self.args.custom_layer_parameters:\n","            layer_number = group.pop(\"layer\")\n","            layer = f\"layer.{layer_number}.\"\n","            group_d = {**group}\n","            group_nd = {**group}\n","            group_nd[\"weight_decay\"] = 0.0\n","            params_d = []\n","            params_nd = []\n","            for n, p in model.named_parameters():\n","                if n not in custom_parameter_names and layer in n:\n","                    if any(nd in n for nd in no_decay):\n","                        params_nd.append(p)\n","                    else:\n","                        params_d.append(p)\n","                    custom_parameter_names.add(n)\n","            group_d[\"params\"] = params_d\n","            group_nd[\"params\"] = params_nd\n","\n","            optimizer_grouped_parameters.append(group_d)\n","            optimizer_grouped_parameters.append(group_nd)\n","\n","        if not self.args.train_custom_parameters_only:\n","            optimizer_grouped_parameters.extend(\n","                [\n","                    {\n","                        \"params\": [\n","                            p\n","                            for n, p in model.named_parameters()\n","                            if n not in custom_parameter_names\n","                            and not any(nd in n for nd in no_decay)\n","                        ],\n","                        \"weight_decay\": args.weight_decay,\n","                    },\n","                    {\n","                        \"params\": [\n","                            p\n","                            for n, p in model.named_parameters()\n","                            if n not in custom_parameter_names\n","                            and any(nd in n for nd in no_decay)\n","                        ],\n","                        \"weight_decay\": 0.0,\n","                    },\n","                ]\n","            )\n","\n","        warmup_steps = math.ceil(t_total * args.warmup_ratio)\n","        args.warmup_steps = (\n","            warmup_steps if args.warmup_steps == 0 else args.warmup_steps\n","        )\n","        if self.args.layering>=0 and self.args.layering<=11:\n","            layerim= self.args.layering\n","            print(\"yep will change the layer\")\n","            if self.args.model_type ==\"roberta\": \n","                #print(\"yep roberta\")\n","                for param in model.roberta.encoder.layer.parameters(): #herkes frozen\n","                    param.requires_grad = False\n","                model.roberta.encoder.layer[layerim].reguires_grad =True\n","                for param in model.roberta.encoder.layer[layerim].parameters(): #herkes frozen\n","                    param.requires_grad = True\n","            if self.args.model_type ==\"bert\": \n","                #print(\"yep bert\")\n","                for param in model.bert.encoder.layer.parameters(): #herkes frozen\n","                    param.requires_grad = False\n","                model.bert.encoder.layer[layerim].reguires_grad =True\n","                for param in model.bert.encoder.layer[layerim].parameters(): #herkes frozen\n","                    param.requires_grad = True\n","            if self.args.model_type ==\"xlnet\": \n","                for param in model.transformer.layer.parameters(): #herkes frozen\n","                    param.requires_grad = False\n","                model.transformer.layer[layerim].reguires_grad =True\n","                for param in model.transformer.layer[layerim].parameters(): #herkes frozen\n","                    param.requires_grad = True\n","            \n","        if args.optimizer == \"AdamW\":\n","            #print(\"it is ADam!\")\n","            optimizer = AdamW(\n","                optimizer_grouped_parameters,\n","                lr=args.learning_rate,\n","                eps=args.adam_epsilon,\n","            )\n","        elif args.optimizer == \"Adafactor\":\n","            optimizer = Adafactor(\n","                optimizer_grouped_parameters,\n","                lr=args.learning_rate,\n","                eps=args.adafactor_eps,\n","                clip_threshold=args.adafactor_clip_threshold,\n","                decay_rate=args.adafactor_decay_rate,\n","                beta1=args.adafactor_beta1,\n","                weight_decay=args.weight_decay,\n","                scale_parameter=args.adafactor_scale_parameter,\n","                relative_step=args.adafactor_relative_step,\n","                warmup_init=args.adafactor_warmup_init,\n","            )\n","            print(\"Using Adafactor for T5\")\n","        elif args.optimizer == \"gu\":\n","            optimizer = torch.optim.SGD(params =  model.parameters(), lr=1e-5)\n","            ###\n","            if self.args.model_type !='xlnet':\n","                if self.args.model_type =='roberta':\n","                    for param in model.roberta.encoder.layer.parameters(): #herkes frozen\n","                        param.requires_grad = False\n","                else:\n","                    for param in model.bert.encoder.layer.parameters(): #herkes frozen\n","                        param.requires_grad = False\n","            else:\n","                for param in model.transformer.layer.parameters(): #herkes frozen\n","                    param.requires_grad = False\n","        elif args.optimizer == \"dft\":\n","            if self.args.model_type !='xlnet':\n","                if self.args.model_type =='roberta':\n","                    optimizer, _ = self.roberta_base_AdamW_LLRD2(self.model,1)\n","                else:\n","                    optimizer, _ = self.roberta_base_AdamW_LLRD2(self.model,2)\n","            else:\n","              optimizer, _ = xlnet_base_AdamW_LLRD(self.model)\n","        elif args.optimizer == \"all\":\n","            if self.args.model_type !='xlnet':\n","                if self.args.model_type =='roberta':\n","                    optimizer, _ = self.roberta_base_AdamW_LLRD2(self.model,1)\n","                    for param in model.roberta.encoder.layer.parameters(): #herkes frozen\n","                        param.requires_grad = False\n","                else:\n","                    optimizer, _ = self.roberta_base_AdamW_LLRD2(self.model,2)\n","                    for param in model.bert.encoder.layer.parameters(): #herkes frozen\n","                        param.requires_grad = False\n","            else:\n","                optimizer, _ = xlnet_base_AdamW_LLRD(self.model)\n","                for param in model.transformer.layer.parameters(): #herkes frozen\n","                    param.requires_grad = False\n","        else:\n","            raise ValueError(\n","                \"{} is not a valid optimizer class. Please use one of ('AdamW', 'Adafactor') instead.\".format(\n","                    args.optimizer\n","                )\n","            )\n","\n","        if args.scheduler == \"constant_schedule\":\n","            scheduler = get_constant_schedule(optimizer)\n","\n","        elif args.scheduler == \"constant_schedule_with_warmup\":\n","            scheduler = get_constant_schedule_with_warmup(\n","                optimizer, num_warmup_steps=args.warmup_steps\n","            )\n","\n","        elif args.scheduler == \"linear_schedule_with_warmup\":\n","            scheduler = get_linear_schedule_with_warmup(\n","                optimizer,\n","                num_warmup_steps=args.warmup_steps,\n","                num_training_steps=t_total,\n","            )\n","\n","        elif args.scheduler == \"cosine_schedule_with_warmup\":\n","            scheduler = get_cosine_schedule_with_warmup(\n","                optimizer,\n","                num_warmup_steps=args.warmup_steps,\n","                num_training_steps=t_total,\n","                num_cycles=args.cosine_schedule_num_cycles,\n","            )\n","\n","        elif args.scheduler == \"cosine_with_hard_restarts_schedule_with_warmup\":\n","            scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n","                optimizer,\n","                num_warmup_steps=args.warmup_steps,\n","                num_training_steps=t_total,\n","                num_cycles=args.cosine_schedule_num_cycles,\n","            )\n","\n","        elif args.scheduler == \"polynomial_decay_schedule_with_warmup\":\n","            scheduler = get_polynomial_decay_schedule_with_warmup(\n","                optimizer,\n","                num_warmup_steps=args.warmup_steps,\n","                num_training_steps=t_total,\n","                lr_end=args.polynomial_decay_schedule_lr_end,\n","                power=args.polynomial_decay_schedule_power,\n","            )\n","        elif args.scheduler ==\"stlr\":\n","            scheduler = STLR(optimizer,max_mul =0.1, ratio =0.38,steps_per_cycle=len(train_dataloader))\n","\n","        else:\n","            raise ValueError(\"{} is not a valid scheduler.\".format(args.scheduler))\n","        #print(optimizer)\n","        #model = self.model\n","        if args.n_gpu > 1:\n","            model = torch.nn.DataParallel(model)\n","\n","        global_step = 0\n","        training_progress_scores = None\n","        tr_loss, logging_loss = 0.0, 0.0\n","        model.zero_grad()\n","        train_iterator = trange(\n","            int(args.num_train_epochs), desc=\"Epoch\", disable=args.silent, mininterval=0\n","        )\n","        epoch_number = 0\n","        best_eval_metric = None\n","        early_stopping_counter = 0\n","        steps_trained_in_current_epoch = 0\n","        epochs_trained = 0\n","        current_loss = \"Initializing\"\n","\n","        if args.model_name and os.path.exists(args.model_name):\n","            try:\n","                # set global_step to gobal_step of last saved checkpoint from model path\n","                checkpoint_suffix = args.model_name.split(\"/\")[-1].split(\"-\")\n","                if len(checkpoint_suffix) > 2:\n","                    checkpoint_suffix = checkpoint_suffix[1]\n","                else:\n","                    checkpoint_suffix = checkpoint_suffix[-1]\n","                global_step = int(checkpoint_suffix)\n","                epochs_trained = global_step // (\n","                    len(train_dataloader) // args.gradient_accumulation_steps\n","                )\n","                steps_trained_in_current_epoch = global_step % (\n","                    len(train_dataloader) // args.gradient_accumulation_steps\n","                )\n","\n","                logger.info(\n","                    \"   Continuing training from checkpoint, will skip to saved global_step\"\n","                )\n","                logger.info(\"   Continuing training from epoch %d\", epochs_trained)\n","                logger.info(\"   Continuing training from global step %d\", global_step)\n","                logger.info(\n","                    \"   Will skip the first %d steps in the current epoch\",\n","                    steps_trained_in_current_epoch,\n","                )\n","            except ValueError:\n","                logger.info(\"   Starting fine-tuning.\")\n","\n","        if args.evaluate_during_training:\n","            training_progress_scores = self._create_training_progress_scores(\n","                multi_label, **kwargs\n","            )\n","\n","        if args.wandb_project:\n","            if not wandb.setup().settings.sweep_id:\n","                logger.info(\" Initializing WandB run for training.\")\n","                wandb.init(\n","                    project=args.wandb_project,\n","                    config={**asdict(args)},\n","                    **args.wandb_kwargs,\n","                )\n","                wandb.run._label(repo=\"simpletransformers\")\n","                self.wandb_run_id = wandb.run.id\n","            wandb.watch(self.model)\n","\n","        if self.args.fp16:\n","            from torch.cuda import amp\n","\n","            scaler = amp.GradScaler()\n","\n","        for _ in train_iterator:\n","            model.train()\n","            if epochs_trained > 0:\n","                epochs_trained -= 1\n","                continue\n","            train_iterator.set_description(\n","                f\"Epoch {epoch_number + 1} of {args.num_train_epochs}\"\n","            )\n","            batch_iterator = tqdm(\n","                train_dataloader,\n","                desc=f\"Running Epoch {epoch_number} of {args.num_train_epochs}\",\n","                disable=args.silent,\n","                mininterval=0,\n","            )\n","            count=0\n","            for step, batch in enumerate(batch_iterator):\n","                if steps_trained_in_current_epoch > 0:\n","                    steps_trained_in_current_epoch -= 1\n","                    continue\n","\n","                inputs = self._get_inputs_dict(batch)\n","                if self.args.fp16:\n","                    with amp.autocast():\n","                        loss, *_ = self._calculate_loss(\n","                            model,\n","                            inputs,\n","                            loss_fct=self.loss_fct,\n","                            num_labels=self.num_labels,\n","                            args=self.args,\n","                        )\n","                else:\n","                    loss, *_ = self._calculate_loss(\n","                        model,\n","                        inputs,\n","                        loss_fct=self.loss_fct,\n","                        num_labels=self.num_labels,\n","                        args=self.args,\n","                    )\n","\n","                if args.n_gpu > 1:\n","                    loss = (\n","                        loss.mean()\n","                    )  # mean() to average on multi-gpu parallel training\n","\n","                current_loss = loss.item()\n","\n","                if show_running_loss:\n","                    batch_iterator.set_description(\n","                        f\"Epochs {epoch_number}/{args.num_train_epochs}. Running Loss: {current_loss:9.4f}\"\n","                    )\n","\n","                if args.gradient_accumulation_steps > 1:\n","                    loss = loss / args.gradient_accumulation_steps\n","\n","                if self.args.fp16:\n","                    scaler.scale(loss).backward()\n","                else:\n","                    loss.backward()\n","\n","                tr_loss += loss.item()\n","                if (step + 1) % args.gradient_accumulation_steps == 0:\n","                    if self.args.fp16:\n","                        scaler.unscale_(optimizer)\n","                    if args.optimizer == \"AdamW\":\n","                        torch.nn.utils.clip_grad_norm_(\n","                            model.parameters(), args.max_grad_norm\n","                        )\n","\n","                    if self.args.fp16:\n","                        scaler.step(optimizer)\n","                        scaler.update()\n","                    else:\n","                        optimizer.step()\n","                    #print(optimizer)\n","                    #optimizer.step()\n","                    scheduler.step()  # Update learning rate schedule\n","                    model.zero_grad()\n","                    global_step += 1\n","                    if args.optimizer ==\"gu\" or args.optimizer==\"all\":\n","                        if self.args.model_type==\"roberta\":\n","                            if count%20==0:\n","                                if layer_num < len(model.roberta.encoder.layer):\n","                                    for param in model.roberta.encoder.layer[layer_num].parameters(): #herkes frozen\n","                                      param.requires_grad=True\n","                                    model.roberta.encoder.layer[layer_num].requires_grad = True\n","                                    layer_num+=1\n","                        elif self.args.model_type ==\"bert\":\n","                            if count%20==0:\n","                                if layer_num < len(model.bert.encoder.layer):\n","                                    model.bert.encoder.layer[layer_num].requires_grad = True\n","                                    for param in model.bert.encoder.layer[layer_num].parameters(): #herkes frozen\n","                                      param.requires_grad=True\n","                                    \n","                                    layer_num+=1\n","                        else:\n","                            if count%20==0:\n","                                if layer_num < len(model.transformer.layer):\n","                                    model.transformer.layer[layer_num].requires_grad = True\n","                                    layer_num+=1\n","                        count+=1                        \n","                    if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n","                        # Log metrics\n","                        tb_writer.add_scalar(\n","                            \"lr\", scheduler.get_last_lr()[0], global_step\n","                        )\n","                        tb_writer.add_scalar(\n","                            \"loss\",\n","                            (tr_loss - logging_loss) / args.logging_steps,\n","                            global_step,\n","                        )\n","                        logging_loss = tr_loss\n","                        if args.wandb_project or self.is_sweeping:\n","                            wandb.log(\n","                                {\n","                                    \"Training loss\": current_loss,\n","                                    \"lr\": scheduler.get_last_lr()[0],\n","                                    \"global_step\": global_step,\n","                                }\n","                            )\n","\n","                    if args.save_steps > 0 and global_step % args.save_steps == 0:\n","                        # Save model checkpoint\n","                        output_dir_current = os.path.join(\n","                            output_dir, \"checkpoint-{}\".format(global_step)\n","                        )\n","\n","                        self.save_model(\n","                            output_dir_current, optimizer, scheduler, model=model\n","                        )\n","\n","                    if args.evaluate_during_training and (\n","                        args.evaluate_during_training_steps > 0\n","                        and global_step % args.evaluate_during_training_steps == 0\n","                    ):\n","                        # Only evaluate when single GPU otherwise metrics may not average well\n","                        results, _, _ = self.eval_model(\n","                            eval_df,\n","                            verbose=verbose and args.evaluate_during_training_verbose,\n","                            silent=args.evaluate_during_training_silent,\n","                            wandb_log=False,\n","                            **kwargs,\n","                        )\n","\n","                        output_dir_current = os.path.join(\n","                            output_dir, \"checkpoint-{}\".format(global_step)\n","                        )\n","\n","                        if args.save_eval_checkpoints:\n","                            self.save_model(\n","                                output_dir_current,\n","                                optimizer,\n","                                scheduler,\n","                                model=model,\n","                                results=results,\n","                            )\n","\n","                        training_progress_scores[\"global_step\"].append(global_step)\n","                        training_progress_scores[\"train_loss\"].append(current_loss)\n","                        for key in results:\n","                            training_progress_scores[key].append(results[key])\n","\n","                        if test_df is not None:\n","                            test_results, _, _ = self.eval_model(\n","                                test_df,\n","                                verbose=verbose\n","                                and args.evaluate_during_training_verbose,\n","                                silent=args.evaluate_during_training_silent,\n","                                wandb_log=False,\n","                                **kwargs,\n","                            )\n","                            for key in test_results:\n","                                training_progress_scores[\"test_\" + key].append(\n","                                    test_results[key]\n","                                )\n","\n","                        report = pd.DataFrame(training_progress_scores)\n","                        report.to_csv(\n","                            os.path.join(\n","                                args.output_dir, \"training_progress_scores.csv\"\n","                            ),\n","                            index=False,\n","                        )\n","\n","                        if args.wandb_project or self.is_sweeping:\n","                            wandb.log(self._get_last_metrics(training_progress_scores))\n","\n","                        for key, value in flatten_results(\n","                            self._get_last_metrics(training_progress_scores)\n","                        ):\n","                            try:\n","                                tb_writer.add_scalar(key, value, global_step)\n","                            except (NotImplementedError, AssertionError):\n","                                if verbose:\n","                                    logger.warning(\n","                                        f\"can't log value of type: {type(value)} to tensorboar\"\n","                                    )\n","                        tb_writer.flush()\n","\n","                        if not best_eval_metric:\n","                            best_eval_metric = results[args.early_stopping_metric]\n","                            self.save_model(\n","                                args.best_model_dir,\n","                                optimizer,\n","                                scheduler,\n","                                model=model,\n","                                results=results,\n","                            )\n","                        if best_eval_metric and args.early_stopping_metric_minimize:\n","                            if (\n","                                best_eval_metric - results[args.early_stopping_metric]\n","                                > args.early_stopping_delta\n","                            ):\n","                                best_eval_metric = results[args.early_stopping_metric]\n","                                self.save_model(\n","                                    args.best_model_dir,\n","                                    optimizer,\n","                                    scheduler,\n","                                    model=model,\n","                                    results=results,\n","                                )\n","                                early_stopping_counter = 0\n","                            else:\n","                                if args.use_early_stopping:\n","                                    if (\n","                                        early_stopping_counter\n","                                        < args.early_stopping_patience\n","                                    ):\n","                                        early_stopping_counter += 1\n","                                        if verbose:\n","                                            logger.info(\n","                                                f\" No improvement in {args.early_stopping_metric}\"\n","                                            )\n","                                            logger.info(\n","                                                f\" Current step: {early_stopping_counter}\"\n","                                            )\n","                                            logger.info(\n","                                                f\" Early stopping patience: {args.early_stopping_patience}\"\n","                                            )\n","                                    else:\n","                                        if verbose:\n","                                            logger.info(\n","                                                f\" Patience of {args.early_stopping_patience} steps reached\"\n","                                            )\n","                                            logger.info(\" Training terminated.\")\n","                                            train_iterator.close()\n","                                        return (\n","                                            global_step,\n","                                            tr_loss / global_step\n","                                            if not self.args.evaluate_during_training\n","                                            else training_progress_scores,\n","                                        )\n","                        else:\n","                            if (\n","                                results[args.early_stopping_metric] - best_eval_metric\n","                                > args.early_stopping_delta\n","                            ):\n","                                best_eval_metric = results[args.early_stopping_metric]\n","                                self.save_model(\n","                                    args.best_model_dir,\n","                                    optimizer,\n","                                    scheduler,\n","                                    model=model,\n","                                    results=results,\n","                                )\n","                                early_stopping_counter = 0\n","                            else:\n","                                if args.use_early_stopping:\n","                                    if (\n","                                        early_stopping_counter\n","                                        < args.early_stopping_patience\n","                                    ):\n","                                        early_stopping_counter += 1\n","                                        if verbose:\n","                                            logger.info(\n","                                                f\" No improvement in {args.early_stopping_metric}\"\n","                                            )\n","                                            logger.info(\n","                                                f\" Current step: {early_stopping_counter}\"\n","                                            )\n","                                            logger.info(\n","                                                f\" Early stopping patience: {args.early_stopping_patience}\"\n","                                            )\n","                                    else:\n","                                        if verbose:\n","                                            logger.info(\n","                                                f\" Patience of {args.early_stopping_patience} steps reached\"\n","                                            )\n","                                            logger.info(\" Training terminated.\")\n","                                            train_iterator.close()\n","                                        return (\n","                                            global_step,\n","                                            tr_loss / global_step\n","                                            if not self.args.evaluate_during_training\n","                                            else training_progress_scores,\n","                                        )\n","                        model.train()\n","            #print(optimizer)\n","    \n","            from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n","\n","            def f1_multiclass2(labels, preds):\n","                return f1_score(labels, preds, average='macro')\n","            def f1_multiclass3(labels, preds):\n","                return f1_score(labels, preds, average='weighted')\n","            def prec_score(labels, preds):\n","                return precision_score(labels, preds, average='micro')\n","            def prec_score2(labels, preds):\n","                return precision_score(labels, preds, average='macro')\n","            def prec_score3(labels, preds):\n","                return precision_score(labels, preds, average='weighted')\n","            def recall(labels, preds):\n","                return recall_score(labels, preds, average='micro')\n","            def recall2(labels, preds):\n","                return recall_score(labels, preds, average='macro')\n","            def recall3(labels, preds):\n","                return recall_score(labels, preds, average='weighted') \n","            #res, bal,bla = self.eval_model(another_df,acc=accuracy_score, f1=f1_multiclass2, prec=prec_score2, recall = recall2)                       \n","            #print(res)\n","            epoch_number += 1\n","            output_dir_current = os.path.join(\n","                output_dir, \"checkpoint-{}-epoch-{}\".format(global_step, epoch_number)\n","            )\n","\n","            if args.save_model_every_epoch or args.evaluate_during_training:\n","                os.makedirs(output_dir_current, exist_ok=True)\n","\n","            if args.save_model_every_epoch:\n","                self.save_model(output_dir_current, optimizer, scheduler, model=model)\n","\n","            if args.evaluate_during_training and args.evaluate_each_epoch:\n","                results, _, _ = self.eval_model(\n","                    eval_df,\n","                    verbose=verbose and args.evaluate_during_training_verbose,\n","                    silent=args.evaluate_during_training_silent,\n","                    wandb_log=False,\n","                    **kwargs,\n","                )\n","\n","                self.save_model(\n","                    output_dir_current, optimizer, scheduler, results=results\n","                )\n","\n","                training_progress_scores[\"global_step\"].append(global_step)\n","                training_progress_scores[\"train_loss\"].append(current_loss)\n","                for key in results:\n","                    training_progress_scores[key].append(results[key])\n","                if test_df is not None:\n","                    test_results, _, _ = self.eval_model(\n","                        test_df,\n","                        verbose=verbose and args.evaluate_during_training_verbose,\n","                        silent=args.evaluate_during_training_silent,\n","                        wandb_log=False,\n","                        **kwargs,\n","                    )\n","                    for key in test_results:\n","                        training_progress_scores[\"test_\" + key].append(\n","                            test_results[key]\n","                        )\n","\n","                report = pd.DataFrame(training_progress_scores)\n","                report.to_csv(\n","                    os.path.join(args.output_dir, \"training_progress_scores.csv\"),\n","                    index=False,\n","                )\n","\n","                if args.wandb_project or self.is_sweeping:\n","                    wandb.log(self._get_last_metrics(training_progress_scores))\n","\n","                for key, value in flatten_results(\n","                    self._get_last_metrics(training_progress_scores)\n","                ):\n","                    try:\n","                        tb_writer.add_scalar(key, value, global_step)\n","                    except (NotImplementedError, AssertionError):\n","                        if verbose:\n","                            logger.warning(\n","                                f\"can't log value of type: {type(value)} to tensorboar\"\n","                            )\n","                tb_writer.flush()\n","\n","                if not best_eval_metric:\n","                    best_eval_metric = results[args.early_stopping_metric]\n","                    self.save_model(\n","                        args.best_model_dir,\n","                        optimizer,\n","                        scheduler,\n","                        model=model,\n","                        results=results,\n","                    )\n","                if best_eval_metric and args.early_stopping_metric_minimize:\n","                    if (\n","                        best_eval_metric - results[args.early_stopping_metric]\n","                        > args.early_stopping_delta\n","                    ):\n","                        best_eval_metric = results[args.early_stopping_metric]\n","                        self.save_model(\n","                            args.best_model_dir,\n","                            optimizer,\n","                            scheduler,\n","                            model=model,\n","                            results=results,\n","                        )\n","                        early_stopping_counter = 0\n","                    else:\n","                        if (\n","                            args.use_early_stopping\n","                            and args.early_stopping_consider_epochs\n","                        ):\n","                            if early_stopping_counter < args.early_stopping_patience:\n","                                early_stopping_counter += 1\n","                                if verbose:\n","                                    logger.info(\n","                                        f\" No improvement in {args.early_stopping_metric}\"\n","                                    )\n","                                    logger.info(\n","                                        f\" Current step: {early_stopping_counter}\"\n","                                    )\n","                                    logger.info(\n","                                        f\" Early stopping patience: {args.early_stopping_patience}\"\n","                                    )\n","                            else:\n","                                if verbose:\n","                                    logger.info(\n","                                        f\" Patience of {args.early_stopping_patience} steps reached\"\n","                                    )\n","                                    logger.info(\" Training terminated.\")\n","                                    train_iterator.close()\n","                                return (\n","                                    global_step,\n","                                    tr_loss / global_step\n","                                    if not self.args.evaluate_during_training\n","                                    else training_progress_scores,\n","                                )\n","                else:\n","                    if (\n","                        results[args.early_stopping_metric] - best_eval_metric\n","                        > args.early_stopping_delta\n","                    ):\n","                        best_eval_metric = results[args.early_stopping_metric]\n","                        self.save_model(\n","                            args.best_model_dir,\n","                            optimizer,\n","                            scheduler,\n","                            model=model,\n","                            results=results,\n","                        )\n","                        early_stopping_counter = 0\n","                    else:\n","                        if (\n","                            args.use_early_stopping\n","                            and args.early_stopping_consider_epochs\n","                        ):\n","                            if early_stopping_counter < args.early_stopping_patience:\n","                                early_stopping_counter += 1\n","                                if verbose:\n","                                    logger.info(\n","                                        f\" No improvement in {args.early_stopping_metric}\"\n","                                    )\n","                                    logger.info(\n","                                        f\" Current step: {early_stopping_counter}\"\n","                                    )\n","                                    logger.info(\n","                                        f\" Early stopping patience: {args.early_stopping_patience}\"\n","                                    )\n","                            else:\n","                                if verbose:\n","                                    logger.info(\n","                                        f\" Patience of {args.early_stopping_patience} steps reached\"\n","                                    )\n","                                    logger.info(\" Training terminated.\")\n","                                    train_iterator.close()\n","                                return (\n","                                    global_step,\n","                                    tr_loss / global_step\n","                                    if not self.args.evaluate_during_training\n","                                    else training_progress_scores,\n","                                )\n","\n","        return (\n","            global_step,\n","            tr_loss / global_step\n","            if not self.args.evaluate_during_training\n","            else training_progress_scores,\n","        )\n","\n","    def eval_model(\n","        self,\n","        eval_df,\n","        multi_label=False,\n","        output_dir=None,\n","        verbose=True,\n","        silent=False,\n","        wandb_log=True,\n","        **kwargs,\n","    ):\n","        \"\"\"\n","        Evaluates the model on eval_df. Saves results to output_dir.\n","        Args:\n","            eval_df: Pandas Dataframe containing at least two columns. If the Dataframe has a header, it should contain a 'text' and a 'labels' column. If no header is present,\n","            the Dataframe should contain at least two columns, with the first column containing the text, and the second column containing the label. The model will be evaluated on this Dataframe.\n","            output_dir: The directory where model files will be saved. If not given, self.args.output_dir will be used.\n","            verbose: If verbose, results will be printed to the console on completion of evaluation.\n","            silent: If silent, tqdm progress bars will be hidden.\n","            wandb_log: If True, evaluation results will be logged to wandb.\n","            **kwargs: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use). E.g. f1=sklearn.metrics.f1_score.\n","                        A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions.\n","        Returns:\n","            result: Dictionary containing evaluation results.\n","            model_outputs: List of model outputs for each row in eval_df\n","            wrong_preds: List of InputExample objects corresponding to each incorrect prediction by the model\n","        \"\"\"  # noqa: ignore flake8\"\n","\n","        if not output_dir:\n","            output_dir = self.args.output_dir\n","\n","        self._move_model_to_device()\n","\n","        result, model_outputs, wrong_preds = self.evaluate(\n","            eval_df,\n","            output_dir,\n","            multi_label=multi_label,\n","            verbose=verbose,\n","            silent=silent,\n","            wandb_log=wandb_log,\n","            **kwargs,\n","        )\n","        self.results.update(result)\n","\n","        if verbose:\n","            logger.info(self.results)\n","\n","        return result, model_outputs, wrong_preds\n","\n","    def evaluate(\n","        self,\n","        eval_df,\n","        output_dir,\n","        multi_label=False,\n","        prefix=\"\",\n","        verbose=True,\n","        silent=False,\n","        wandb_log=True,\n","        **kwargs,\n","    ):\n","        \"\"\"\n","        Evaluates the model on eval_df.\n","        Utility function to be used by the eval_model() method. Not intended to be used directly.\n","        \"\"\"\n","\n","        model = self.model\n","        args = self.args\n","        eval_output_dir = output_dir\n","\n","        results = {}\n","        if self.args.use_hf_datasets:\n","            if self.args.sliding_window:\n","                raise ValueError(\n","                    \"HuggingFace Datasets cannot be used with sliding window.\"\n","                )\n","            if self.args.model_type == \"layoutlm\":\n","                raise NotImplementedError(\n","                    \"HuggingFace Datasets support is not implemented for LayoutLM models\"\n","                )\n","            eval_dataset = load_hf_dataset(\n","                eval_df, self.tokenizer, self.args, multi_label=multi_label\n","            )\n","            eval_examples = None\n","        elif isinstance(eval_df, str) and self.args.lazy_loading:\n","            if self.args.model_type == \"layoutlm\":\n","                raise NotImplementedError(\n","                    \"Lazy loading is not implemented for LayoutLM models\"\n","                )\n","            eval_dataset = LazyClassificationDataset(eval_df, self.tokenizer, self.args)\n","            eval_examples = None\n","        else:\n","            if self.args.lazy_loading:\n","                raise ValueError(\n","                    \"Input must be given as a path to a file when using lazy loading\"\n","                )\n","\n","            if \"text\" in eval_df.columns and \"labels\" in eval_df.columns:\n","                if self.args.model_type == \"layoutlm\":\n","                    eval_examples = [\n","                        InputExample(i, text, None, label, x0, y0, x1, y1)\n","                        for i, (text, label, x0, y0, x1, y1) in enumerate(\n","                            zip(\n","                                eval_df[\"text\"].astype(str),\n","                                eval_df[\"labels\"],\n","                                eval_df[\"x0\"],\n","                                eval_df[\"y0\"],\n","                                eval_df[\"x1\"],\n","                                eval_df[\"y1\"],\n","                            )\n","                        )\n","                    ]\n","                else:\n","                    eval_examples = (\n","                        eval_df[\"text\"].astype(str).tolist(),\n","                        eval_df[\"labels\"].tolist(),\n","                    )\n","            elif \"text_a\" in eval_df.columns and \"text_b\" in eval_df.columns:\n","                if self.args.model_type == \"layoutlm\":\n","                    raise ValueError(\"LayoutLM cannot be used with sentence-pair tasks\")\n","                else:\n","                    eval_examples = (\n","                        eval_df[\"text_a\"].astype(str).tolist(),\n","                        eval_df[\"text_b\"].astype(str).tolist(),\n","                        eval_df[\"labels\"].tolist(),\n","                    )\n","            else:\n","                warnings.warn(\n","                    \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n","                )\n","                eval_examples = (\n","                    eval_df.iloc[:, 0].astype(str).tolist(),\n","                    eval_df.iloc[:, 1].tolist(),\n","                )\n","\n","            if args.sliding_window:\n","                eval_dataset, window_counts = self.load_and_cache_examples(\n","                    eval_examples, evaluate=True, verbose=verbose, silent=silent\n","                )\n","            else:\n","                eval_dataset = self.load_and_cache_examples(\n","                    eval_examples, evaluate=True, verbose=verbose, silent=silent\n","                )\n","        os.makedirs(eval_output_dir, exist_ok=True)\n","\n","        eval_sampler = SequentialSampler(eval_dataset)\n","        eval_dataloader = DataLoader(\n","            eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size\n","        )\n","\n","        if args.n_gpu > 1:\n","            model = torch.nn.DataParallel(model)\n","\n","        eval_loss = 0.0\n","        nb_eval_steps = 0\n","        n_batches = len(eval_dataloader)\n","        preds = np.empty((len(eval_dataset), self.num_labels))\n","        if multi_label:\n","            out_label_ids = np.empty((len(eval_dataset), self.num_labels))\n","        else:\n","            out_label_ids = np.empty((len(eval_dataset)))\n","        model.eval()\n","\n","        if self.args.fp16:\n","            from torch.cuda import amp\n","\n","        for i, batch in enumerate(\n","            tqdm(\n","                eval_dataloader,\n","                disable=args.silent or silent,\n","                desc=\"Running Evaluation\",\n","            )\n","        ):\n","            # batch = tuple(t.to(device) for t in batch)\n","\n","            with torch.no_grad():\n","                inputs = self._get_inputs_dict(batch)\n","\n","                if self.args.fp16:\n","                    with amp.autocast():\n","                        outputs = self._calculate_loss(\n","                            model,\n","                            inputs,\n","                            loss_fct=self.loss_fct,\n","                            num_labels=self.num_labels,\n","                            args=self.args,\n","                        )\n","                        tmp_eval_loss, logits = outputs[:2]\n","                else:\n","                    outputs = self._calculate_loss(\n","                        model,\n","                        inputs,\n","                        loss_fct=self.loss_fct,\n","                        num_labels=self.num_labels,\n","                        args=self.args,\n","                    )\n","                    tmp_eval_loss, logits = outputs[:2]\n","\n","                if multi_label:\n","                    logits = logits.sigmoid()\n","                if self.args.n_gpu > 1:\n","                    tmp_eval_loss = tmp_eval_loss.mean()\n","                eval_loss += tmp_eval_loss.item()\n","\n","            nb_eval_steps += 1\n","\n","            start_index = self.args.eval_batch_size * i\n","            end_index = (\n","                start_index + self.args.eval_batch_size\n","                if i != (n_batches - 1)\n","                else len(eval_dataset)\n","            )\n","            preds[start_index:end_index] = logits.detach().cpu().numpy()\n","            out_label_ids[start_index:end_index] = (\n","                inputs[\"labels\"].detach().cpu().numpy()\n","            )\n","\n","            # if preds is None:\n","            #     preds = logits.detach().cpu().numpy()\n","            #     out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n","            # else:\n","            #     preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n","            #     out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n","\n","        eval_loss = eval_loss / nb_eval_steps\n","\n","        if args.sliding_window:\n","            count = 0\n","            window_ranges = []\n","            for n_windows in window_counts:\n","                window_ranges.append([count, count + n_windows])\n","                count += n_windows\n","\n","            preds = [\n","                preds[window_range[0] : window_range[1]]\n","                for window_range in window_ranges\n","            ]\n","            out_label_ids = [\n","                out_label_ids[i]\n","                for i in range(len(out_label_ids))\n","                if i in [window[0] for window in window_ranges]\n","            ]\n","\n","            model_outputs = preds\n","\n","            preds = [np.argmax(pred, axis=1) for pred in preds]\n","            final_preds = []\n","            for pred_row in preds:\n","                val_freqs_desc = Counter(pred_row).most_common()\n","                if (\n","                    len(val_freqs_desc) > 1\n","                    and val_freqs_desc[0][1] == val_freqs_desc[1][1]\n","                ):\n","                    final_preds.append(args.tie_value)\n","                else:\n","                    final_preds.append(val_freqs_desc[0][0])\n","            preds = np.array(final_preds)\n","        elif not multi_label and args.regression is True:\n","            preds = np.squeeze(preds)\n","            model_outputs = preds\n","        else:\n","            model_outputs = preds\n","\n","            if not multi_label:\n","                preds = np.argmax(preds, axis=1)\n","\n","        result, wrong = self.compute_metrics(\n","            preds, model_outputs, out_label_ids, eval_examples, **kwargs\n","        )\n","        result[\"eval_loss\"] = eval_loss\n","        results.update(result)\n","\n","        output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n","        with open(output_eval_file, \"w\") as writer:\n","            for key in sorted(result.keys()):\n","                writer.write(\"{} = {}\\n\".format(key, str(result[key])))\n","\n","        if (\n","            self.args.wandb_project\n","            and wandb_log\n","            and not multi_label\n","            and not self.args.regression\n","        ):\n","            if not wandb.setup().settings.sweep_id:\n","                logger.info(\" Initializing WandB run for evaluation.\")\n","                wandb.init(\n","                    project=args.wandb_project,\n","                    config={**asdict(args)},\n","                    **args.wandb_kwargs,\n","                )\n","                wandb.run._label(repo=\"simpletransformers\")\n","            if not args.labels_map:\n","                self.args.labels_map = {i: i for i in range(self.num_labels)}\n","\n","            labels_list = sorted(list(self.args.labels_map.keys()))\n","            inverse_labels_map = {\n","                value: key for key, value in self.args.labels_map.items()\n","            }\n","\n","            truth = [inverse_labels_map[out] for out in out_label_ids]\n","\n","            # Confusion Matrix\n","            wandb.sklearn.plot_confusion_matrix(\n","                truth,\n","                [inverse_labels_map[pred] for pred in preds],\n","                labels=labels_list,\n","            )\n","\n","            if not self.args.sliding_window:\n","                # ROC`\n","                wandb.log({\"roc\": wandb.plots.ROC(truth, model_outputs, labels_list)})\n","\n","                # Precision Recall\n","                wandb.log(\n","                    {\n","                        \"pr\": wandb.plots.precision_recall(\n","                            truth, model_outputs, labels_list\n","                        )\n","                    }\n","                )\n","\n","        return results, model_outputs, wrong\n","\n","    def load_and_cache_examples(\n","        self,\n","        examples,\n","        evaluate=False,\n","        no_cache=False,\n","        multi_label=False,\n","        verbose=True,\n","        silent=False,\n","    ):\n","        \"\"\"\n","        Converts a list of InputExample objects to a TensorDataset containing InputFeatures. Caches the InputFeatures.\n","        Utility function for train() and eval() methods. Not intended to be used directly.\n","        \"\"\"\n","\n","        process_count = self.args.process_count\n","\n","        tokenizer = self.tokenizer\n","        args = self.args\n","\n","        if not no_cache:\n","            no_cache = args.no_cache\n","\n","        if not multi_label and args.regression:\n","            output_mode = \"regression\"\n","        else:\n","            output_mode = \"classification\"\n","\n","        if not no_cache:\n","            os.makedirs(self.args.cache_dir, exist_ok=True)\n","\n","        mode = \"dev\" if evaluate else \"train\"\n","        if args.sliding_window or self.args.model_type == \"layoutlm\":\n","            cached_features_file = os.path.join(\n","                args.cache_dir,\n","                \"cached_{}_{}_{}_{}_{}\".format(\n","                    mode,\n","                    args.model_type,\n","                    args.max_seq_length,\n","                    self.num_labels,\n","                    len(examples),\n","                ),\n","            )\n","\n","            if os.path.exists(cached_features_file) and (\n","                (not args.reprocess_input_data and not no_cache)\n","                or (mode == \"dev\" and args.use_cached_eval_features and not no_cache)\n","            ):\n","                features = torch.load(cached_features_file)\n","                if verbose:\n","                    logger.info(\n","                        f\" Features loaded from cache at {cached_features_file}\"\n","                    )\n","            else:\n","                if verbose:\n","                    logger.info(\" Converting to features started. Cache is not used.\")\n","                    if args.sliding_window:\n","                        logger.info(\" Sliding window enabled\")\n","\n","                if self.args.model_type != \"layoutlm\":\n","                    if len(examples) == 3:\n","                        examples = [\n","                            InputExample(i, text_a, text_b, label)\n","                            for i, (text_a, text_b, label) in enumerate(zip(*examples))\n","                        ]\n","                    else:\n","                        examples = [\n","                            InputExample(i, text_a, None, label)\n","                            for i, (text_a, label) in enumerate(zip(*examples))\n","                        ]\n","\n","                # If labels_map is defined, then labels need to be replaced with ints\n","                if self.args.labels_map and not self.args.regression:\n","                    for example in examples:\n","                        if multi_label:\n","                            example.label = [\n","                                self.args.labels_map[label] for label in example.label\n","                            ]\n","                        else:\n","                            example.label = self.args.labels_map[example.label]\n","\n","                features = convert_examples_to_features(\n","                    examples,\n","                    args.max_seq_length,\n","                    tokenizer,\n","                    output_mode,\n","                    # XLNet has a CLS token at the end\n","                    cls_token_at_end=bool(args.model_type in [\"xlnet\"]),\n","                    cls_token=tokenizer.cls_token,\n","                    cls_token_segment_id=2 if args.model_type in [\"xlnet\"] else 0,\n","                    sep_token=tokenizer.sep_token,\n","                    # RoBERTa uses an extra separator b/w pairs of sentences,\n","                    # cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n","                    sep_token_extra=args.model_type in MODELS_WITH_EXTRA_SEP_TOKEN,\n","                    # PAD on the left for XLNet\n","                    pad_on_left=bool(args.model_type in [\"xlnet\"]),\n","                    pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n","                    pad_token_segment_id=4 if args.model_type in [\"xlnet\"] else 0,\n","                    process_count=process_count,\n","                    multi_label=multi_label,\n","                    silent=args.silent or silent,\n","                    use_multiprocessing=args.use_multiprocessing_for_evaluation,\n","                    sliding_window=args.sliding_window,\n","                    flatten=not evaluate,\n","                    stride=args.stride,\n","                    add_prefix_space=args.model_type in MODELS_WITH_ADD_PREFIX_SPACE,\n","                    # avoid padding in case of single example/online inferencing to decrease execution time\n","                    pad_to_max_length=bool(len(examples) > 1),\n","                    args=args,\n","                )\n","                if verbose and args.sliding_window:\n","                    logger.info(\n","                        f\" {len(features)} features created from {len(examples)} samples.\"\n","                    )\n","\n","                if not no_cache:\n","                    torch.save(features, cached_features_file)\n","\n","            if args.sliding_window and evaluate:\n","                features = [\n","                    [feature_set] if not isinstance(feature_set, list) else feature_set\n","                    for feature_set in features\n","                ]\n","                window_counts = [len(sample) for sample in features]\n","                features = [\n","                    feature for feature_set in features for feature in feature_set\n","                ]\n","\n","            all_input_ids = torch.tensor(\n","                [f.input_ids for f in features], dtype=torch.long\n","            )\n","            all_input_mask = torch.tensor(\n","                [f.input_mask for f in features], dtype=torch.long\n","            )\n","            all_segment_ids = torch.tensor(\n","                [f.segment_ids for f in features], dtype=torch.long\n","            )\n","\n","            if self.args.model_type == \"layoutlm\":\n","                all_bboxes = torch.tensor(\n","                    [f.bboxes for f in features], dtype=torch.long\n","                )\n","\n","            if output_mode == \"classification\":\n","                all_label_ids = torch.tensor(\n","                    [f.label_id for f in features], dtype=torch.long\n","                )\n","            elif output_mode == \"regression\":\n","                all_label_ids = torch.tensor(\n","                    [f.label_id for f in features], dtype=torch.float\n","                )\n","\n","            if self.args.model_type == \"layoutlm\":\n","                dataset = TensorDataset(\n","                    all_input_ids,\n","                    all_input_mask,\n","                    all_segment_ids,\n","                    all_label_ids,\n","                    all_bboxes,\n","                )\n","            else:\n","                dataset = TensorDataset(\n","                    all_input_ids, all_input_mask, all_segment_ids, all_label_ids\n","                )\n","\n","            if args.sliding_window and evaluate:\n","                return dataset, window_counts\n","            else:\n","                return dataset\n","        else:\n","            dataset = ClassificationDataset(\n","                examples,\n","                self.tokenizer,\n","                self.args,\n","                mode=mode,\n","                multi_label=multi_label,\n","                output_mode=output_mode,\n","                no_cache=no_cache,\n","            )\n","            return dataset\n","\n","    def compute_metrics(\n","        self,\n","        preds,\n","        model_outputs,\n","        labels,\n","        eval_examples=None,\n","        multi_label=False,\n","        **kwargs,\n","    ):\n","        \"\"\"\n","        Computes the evaluation metrics for the model predictions.\n","        Args:\n","            preds: Model predictions\n","            model_outputs: Model outputs\n","            labels: Ground truth labels\n","            eval_examples: List of examples on which evaluation was performed\n","            **kwargs: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use). E.g. f1=sklearn.metrics.f1_score.\n","                        A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions.\n","        Returns:\n","            result: Dictionary containing evaluation results.\n","            For non-binary classification, the dictionary format is: (Matthews correlation coefficient, tp, tn, fp, fn).\n","            For binary classification, the dictionary format is: (Matthews correlation coefficient, tp, tn, fp, fn, AUROC, AUPRC).\n","            wrong: List of InputExample objects corresponding to each incorrect prediction by the model\n","        \"\"\"  # noqa: ignore flake8\"\n","\n","        assert len(preds) == len(labels)\n","\n","        extra_metrics = {}\n","        for metric, func in kwargs.items():\n","            if metric.startswith(\"prob_\"):\n","                extra_metrics[metric] = func(labels, model_outputs)\n","            else:\n","                extra_metrics[metric] = func(labels, preds)\n","\n","        if multi_label:\n","            threshold_values = self.args.threshold if self.args.threshold else 0.5\n","            if isinstance(threshold_values, list):\n","                mismatched = labels != [\n","                    [\n","                        self._threshold(pred, threshold_values[i])\n","                        for i, pred in enumerate(example)\n","                    ]\n","                    for example in preds\n","                ]\n","            else:\n","                mismatched = labels != [\n","                    [self._threshold(pred, threshold_values) for pred in example]\n","                    for example in preds\n","                ]\n","        else:\n","            mismatched = labels != preds\n","\n","        if eval_examples:\n","            wrong = [i for (i, v) in zip(eval_examples, mismatched) if v.any()]\n","        else:\n","            wrong = [\"NA\"]\n","\n","        if multi_label:\n","            label_ranking_score = label_ranking_average_precision_score(labels, preds)\n","            return {**{\"LRAP\": label_ranking_score}, **extra_metrics}, wrong\n","        elif self.args.regression:\n","            return {**extra_metrics}, wrong\n","\n","        mcc = matthews_corrcoef(labels, preds)\n","        if self.model.num_labels == 2:\n","            tn, fp, fn, tp = confusion_matrix(labels, preds, labels=[0, 1]).ravel()\n","            if self.args.sliding_window:\n","                return (\n","                    {\n","                        **{\"mcc\": mcc, \"tp\": tp, \"tn\": tn, \"fp\": fp, \"fn\": fn},\n","                        **extra_metrics,\n","                    },\n","                    wrong,\n","                )\n","            else:\n","                scores = np.array([softmax(element)[1] for element in model_outputs])\n","                fpr, tpr, thresholds = roc_curve(labels, scores)\n","                auroc = auc(fpr, tpr)\n","                auprc = average_precision_score(labels, scores)\n","                return (\n","                    {\n","                        **{\n","                            \"mcc\": mcc,\n","                            \"tp\": tp,\n","                            \"tn\": tn,\n","                            \"fp\": fp,\n","                            \"fn\": fn,\n","                            \"auroc\": auroc,\n","                            \"auprc\": auprc,\n","                        },\n","                        **extra_metrics,\n","                    },\n","                    wrong,\n","                )\n","        else:\n","            return {**{\"mcc\": mcc}, **extra_metrics}, wrong\n","\n","    def predict(self, to_predict, multi_label=False):\n","        \"\"\"\n","        Performs predictions on a list of text.\n","        Args:\n","            to_predict: A python list of text (str) to be sent to the model for prediction.\n","        Returns:\n","            preds: A python list of the predictions (0 or 1) for each text.\n","            model_outputs: A python list of the raw model outputs for each text.\n","        \"\"\"\n","\n","        model = self.model\n","        args = self.args\n","\n","        eval_loss = 0.0\n","        nb_eval_steps = 0\n","        preds = np.empty((len(to_predict), self.num_labels))\n","        if multi_label:\n","            out_label_ids = np.empty((len(to_predict), self.num_labels))\n","        else:\n","            out_label_ids = np.empty((len(to_predict)))\n","\n","        if not multi_label and self.args.onnx:\n","            model_inputs = self.tokenizer.batch_encode_plus(\n","                to_predict, return_tensors=\"pt\", padding=True, truncation=True\n","            )\n","\n","            if self.args.model_type in [\"bert\", \"xlnet\", \"albert\", \"layoutlm\"]:\n","                for i, (input_ids, attention_mask, token_type_ids) in enumerate(\n","                    zip(\n","                        model_inputs[\"input_ids\"],\n","                        model_inputs[\"attention_mask\"],\n","                        model_inputs[\"token_type_ids\"],\n","                    )\n","                ):\n","                    input_ids = input_ids.unsqueeze(0).detach().cpu().numpy()\n","                    attention_mask = attention_mask.unsqueeze(0).detach().cpu().numpy()\n","                    token_type_ids = token_type_ids.unsqueeze(0).detach().cpu().numpy()\n","                    inputs_onnx = {\n","                        \"input_ids\": input_ids,\n","                        \"attention_mask\": attention_mask,\n","                        \"token_type_ids\": token_type_ids,\n","                    }\n","\n","                    # Run the model (None = get all the outputs)\n","                    output = self.model.run(None, inputs_onnx)\n","\n","                    preds[i] = output[0]\n","\n","            else:\n","                for i, (input_ids, attention_mask) in enumerate(\n","                    zip(model_inputs[\"input_ids\"], model_inputs[\"attention_mask\"])\n","                ):\n","                    input_ids = input_ids.unsqueeze(0).detach().cpu().numpy()\n","                    attention_mask = attention_mask.unsqueeze(0).detach().cpu().numpy()\n","                    inputs_onnx = {\n","                        \"input_ids\": input_ids,\n","                        \"attention_mask\": attention_mask,\n","                    }\n","\n","                    # Run the model (None = get all the outputs)\n","                    output = self.model.run(None, inputs_onnx)\n","\n","                    preds[i] = output[0]\n","\n","            model_outputs = preds\n","            preds = np.argmax(preds, axis=1)\n","\n","        else:\n","            self._move_model_to_device()\n","            dummy_label = (\n","                0\n","                if not self.args.labels_map\n","                else next(iter(self.args.labels_map.keys()))\n","            )\n","\n","            if multi_label:\n","                dummy_label = [dummy_label for i in range(self.num_labels)]\n","\n","            if args.n_gpu > 1:\n","                model = torch.nn.DataParallel(model)\n","\n","            if isinstance(to_predict[0], list):\n","                eval_examples = (\n","                    *zip(*to_predict),\n","                    [dummy_label for i in range(len(to_predict))],\n","                )\n","            else:\n","                eval_examples = (\n","                    to_predict,\n","                    [dummy_label for i in range(len(to_predict))],\n","                )\n","\n","            if args.sliding_window:\n","                eval_dataset, window_counts = self.load_and_cache_examples(\n","                    eval_examples, evaluate=True, no_cache=True\n","                )\n","                preds = np.empty((len(eval_dataset), self.num_labels))\n","                if multi_label:\n","                    out_label_ids = np.empty((len(eval_dataset), self.num_labels))\n","                else:\n","                    out_label_ids = np.empty((len(eval_dataset)))\n","            else:\n","                eval_dataset = self.load_and_cache_examples(\n","                    eval_examples, evaluate=True, multi_label=multi_label, no_cache=True\n","                )\n","\n","            eval_sampler = SequentialSampler(eval_dataset)\n","            eval_dataloader = DataLoader(\n","                eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size\n","            )\n","\n","            if self.args.fp16:\n","                from torch.cuda import amp\n","\n","            if self.config.output_hidden_states:\n","                model.eval()\n","                preds = None\n","                out_label_ids = None\n","                for i, batch in enumerate(\n","                    tqdm(\n","                        eval_dataloader, disable=args.silent, desc=\"Running Prediction\"\n","                    )\n","                ):\n","                    # batch = tuple(t.to(self.device) for t in batch)\n","                    with torch.no_grad():\n","                        inputs = self._get_inputs_dict(batch, no_hf=True)\n","\n","                        if self.args.fp16:\n","                            with amp.autocast():\n","                                outputs = self._calculate_loss(\n","                                    model,\n","                                    inputs,\n","                                    loss_fct=self.loss_fct,\n","                                    num_labels=self.num_labels,\n","                                    args=self.args,\n","                                )\n","                                tmp_eval_loss, logits = outputs[:2]\n","                        else:\n","                            outputs = self._calculate_loss(\n","                                model,\n","                                inputs,\n","                                loss_fct=self.loss_fct,\n","                                num_labels=self.num_labels,\n","                                args=self.args,\n","                            )\n","                            tmp_eval_loss, logits = outputs[:2]\n","                        embedding_outputs, layer_hidden_states = (\n","                            outputs[2][0],\n","                            outputs[2][1:],\n","                        )\n","\n","                        if multi_label:\n","                            logits = logits.sigmoid()\n","\n","                        if self.args.n_gpu > 1:\n","                            tmp_eval_loss = tmp_eval_loss.mean()\n","                        eval_loss += tmp_eval_loss.item()\n","\n","                    nb_eval_steps += 1\n","\n","                    if preds is None:\n","                        preds = logits.detach().cpu().numpy()\n","                        out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n","                        all_layer_hidden_states = np.array(\n","                            [\n","                                state.detach().cpu().numpy()\n","                                for state in layer_hidden_states\n","                            ]\n","                        )\n","                        all_embedding_outputs = embedding_outputs.detach().cpu().numpy()\n","                    else:\n","                        preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n","                        out_label_ids = np.append(\n","                            out_label_ids,\n","                            inputs[\"labels\"].detach().cpu().numpy(),\n","                            axis=0,\n","                        )\n","                        all_layer_hidden_states = np.append(\n","                            all_layer_hidden_states,\n","                            np.array(\n","                                [\n","                                    state.detach().cpu().numpy()\n","                                    for state in layer_hidden_states\n","                                ]\n","                            ),\n","                            axis=1,\n","                        )\n","                        all_embedding_outputs = np.append(\n","                            all_embedding_outputs,\n","                            embedding_outputs.detach().cpu().numpy(),\n","                            axis=0,\n","                        )\n","            else:\n","                n_batches = len(eval_dataloader)\n","                for i, batch in enumerate(tqdm(eval_dataloader, disable=args.silent)):\n","                    model.eval()\n","                    # batch = tuple(t.to(device) for t in batch)\n","\n","                    with torch.no_grad():\n","                        inputs = self._get_inputs_dict(batch, no_hf=True)\n","\n","                        if self.args.fp16:\n","                            with amp.autocast():\n","                                outputs = self._calculate_loss(\n","                                    model,\n","                                    inputs,\n","                                    loss_fct=self.loss_fct,\n","                                    num_labels=self.num_labels,\n","                                    args=self.args,\n","                                )\n","                                tmp_eval_loss, logits = outputs[:2]\n","                        else:\n","                            outputs = self._calculate_loss(\n","                                model,\n","                                inputs,\n","                                loss_fct=self.loss_fct,\n","                                num_labels=self.num_labels,\n","                                args=self.args,\n","                            )\n","                            tmp_eval_loss, logits = outputs[:2]\n","\n","                        if multi_label:\n","                            logits = logits.sigmoid()\n","\n","                        if self.args.n_gpu > 1:\n","                            tmp_eval_loss = tmp_eval_loss.mean()\n","                        eval_loss += tmp_eval_loss.item()\n","\n","                    nb_eval_steps += 1\n","\n","                    start_index = self.args.eval_batch_size * i\n","                    end_index = (\n","                        start_index + self.args.eval_batch_size\n","                        if i != (n_batches - 1)\n","                        else len(eval_dataset)\n","                    )\n","                    preds[start_index:end_index] = logits.detach().cpu().numpy()\n","                    out_label_ids[start_index:end_index] = (\n","                        inputs[\"labels\"].detach().cpu().numpy()\n","                    )\n","\n","            eval_loss = eval_loss / nb_eval_steps\n","\n","            if args.sliding_window:\n","                count = 0\n","                window_ranges = []\n","                for n_windows in window_counts:\n","                    window_ranges.append([count, count + n_windows])\n","                    count += n_windows\n","\n","                preds = [\n","                    preds[window_range[0] : window_range[1]]\n","                    for window_range in window_ranges\n","                ]\n","\n","                model_outputs = preds\n","\n","                preds = [np.argmax(pred, axis=1) for pred in preds]\n","                final_preds = []\n","                for pred_row in preds:\n","                    mode_pred, counts = mode(pred_row)\n","                    if len(counts) > 1 and counts[0] == counts[1]:\n","                        final_preds.append(args.tie_value)\n","                    else:\n","                        final_preds.append(mode_pred[0])\n","                preds = np.array(final_preds)\n","            elif not multi_label and args.regression is True:\n","                preds = np.squeeze(preds)\n","                model_outputs = preds\n","            else:\n","                model_outputs = preds\n","                if multi_label:\n","                    if isinstance(args.threshold, list):\n","                        threshold_values = args.threshold\n","                        preds = [\n","                            [\n","                                self._threshold(pred, threshold_values[i])\n","                                for i, pred in enumerate(example)\n","                            ]\n","                            for example in preds\n","                        ]\n","                    else:\n","                        preds = [\n","                            [self._threshold(pred, args.threshold) for pred in example]\n","                            for example in preds\n","                        ]\n","                else:\n","                    preds = np.argmax(preds, axis=1)\n","\n","        if self.args.labels_map and not self.args.regression:\n","            inverse_labels_map = {\n","                value: key for key, value in self.args.labels_map.items()\n","            }\n","            preds = [inverse_labels_map[pred] for pred in preds]\n","\n","        if self.config.output_hidden_states:\n","            return preds, model_outputs, all_embedding_outputs, all_layer_hidden_states\n","        else:\n","            return preds, model_outputs\n","\n","    def convert_to_onnx(self, output_dir=None, set_onnx_arg=True):\n","        \"\"\"Convert the model to ONNX format and save to output_dir\n","        Args:\n","            output_dir (str, optional): If specified, ONNX model will be saved to output_dir (else args.output_dir will be used). Defaults to None.\n","            set_onnx_arg (bool, optional): Updates the model args to set onnx=True. Defaults to True.\n","        \"\"\"  # noqa\n","        if not output_dir:\n","            output_dir = os.path.join(self.args.output_dir, \"onnx\")\n","        os.makedirs(output_dir, exist_ok=True)\n","\n","        if os.listdir(output_dir):\n","            raise ValueError(\n","                \"Output directory ({}) already exists and is not empty.\"\n","                \" Output directory for onnx conversion must be empty.\".format(\n","                    output_dir\n","                )\n","            )\n","\n","        onnx_model_name = os.path.join(output_dir, \"onnx_model.onnx\")\n","\n","        with tempfile.TemporaryDirectory() as temp_dir:\n","            self.save_model(output_dir=temp_dir, model=self.model)\n","\n","            convert(\n","                framework=\"pt\",\n","                model=temp_dir,\n","                tokenizer=self.tokenizer,\n","                output=Path(onnx_model_name),\n","                pipeline_name=\"sentiment-analysis\",\n","                opset=11,\n","            )\n","\n","        self.args.onnx = True\n","        self.tokenizer.save_pretrained(output_dir)\n","        self.config.save_pretrained(output_dir)\n","        self.save_model_args(output_dir)\n","\n","    def _calculate_loss(self, model, inputs, loss_fct, num_labels, args):\n","        outputs = model(**inputs)\n","        # model outputs are always tuple in pytorch-transformers (see doc)\n","        loss = outputs[0]\n","        if loss_fct:\n","            logits = outputs[1]\n","            labels = inputs[\"labels\"]\n","\n","            loss = loss_fct(logits.view(-1, num_labels), labels.view(-1))\n","        return (loss, *outputs[1:])\n","\n","    def _threshold(self, x, threshold):\n","        if x >= threshold:\n","            return 1\n","        return 0\n","\n","    def _move_model_to_device(self):\n","        self.model.to(self.device)\n","\n","    def _get_inputs_dict(self, batch, no_hf=False):\n","        if self.args.use_hf_datasets and not no_hf:\n","            return {key: value.to(self.device) for key, value in batch.items()}\n","        if isinstance(batch[0], dict):\n","            inputs = {\n","                key: value.squeeze(1).to(self.device) for key, value in batch[0].items()\n","            }\n","            inputs[\"labels\"] = batch[1].to(self.device)\n","        else:\n","            batch = tuple(t.to(self.device) for t in batch)\n","\n","            inputs = {\n","                \"input_ids\": batch[0],\n","                \"attention_mask\": batch[1],\n","                \"labels\": batch[3],\n","            }\n","\n","            # XLM, DistilBERT and RoBERTa don't use segment_ids\n","            if self.args.model_type != \"distilbert\":\n","                inputs[\"token_type_ids\"] = (\n","                    batch[2]\n","                    if self.args.model_type in [\"bert\", \"xlnet\", \"albert\", \"layoutlm\"]\n","                    else None\n","                )\n","\n","        if self.args.model_type == \"layoutlm\":\n","            inputs[\"bbox\"] = batch[4]\n","\n","        return inputs\n","\n","    def _get_last_metrics(self, metric_values):\n","        return {metric: values[-1] for metric, values in metric_values.items()}\n","\n","    def _create_training_progress_scores(self, multi_label, **kwargs):\n","        return collections.defaultdict(list)\n","        \"\"\"extra_metrics = {key: [] for key in kwargs}\n","        if multi_label:\n","            training_progress_scores = {\n","                \"global_step\": [],\n","                \"LRAP\": [],\n","                \"train_loss\": [],\n","                \"eval_loss\": [],\n","                **extra_metrics,\n","            }\n","        else:\n","            if self.model.num_labels == 2:\n","                if self.args.sliding_window:\n","                    training_progress_scores = {\n","                        \"global_step\": [],\n","                        \"tp\": [],\n","                        \"tn\": [],\n","                        \"fp\": [],\n","                        \"fn\": [],\n","                        \"mcc\": [],\n","                        \"train_loss\": [],\n","                        \"eval_loss\": [],\n","                        **extra_metrics,\n","                    }\n","                else:\n","                    training_progress_scores = {\n","                        \"global_step\": [],\n","                        \"tp\": [],\n","                        \"tn\": [],\n","                        \"fp\": [],\n","                        \"fn\": [],\n","                        \"mcc\": [],\n","                        \"train_loss\": [],\n","                        \"eval_loss\": [],\n","                        \"auroc\": [],\n","                        \"auprc\": [],\n","                        **extra_metrics,\n","                    }\n","            elif self.model.num_labels == 1:\n","                training_progress_scores = {\n","                    \"global_step\": [],\n","                    \"train_loss\": [],\n","                    \"eval_loss\": [],\n","                    **extra_metrics,\n","                }\n","            else:\n","                training_progress_scores = {\n","                    \"global_step\": [],\n","                    \"mcc\": [],\n","                    \"train_loss\": [],\n","                    \"eval_loss\": [],\n","                    **extra_metrics,\n","                }\n","        return training_progress_scores\"\"\"\n","\n","    def save_model(\n","        self, output_dir=None, optimizer=None, scheduler=None, model=None, results=None\n","    ):\n","        if not output_dir:\n","            output_dir = self.args.output_dir\n","        os.makedirs(output_dir, exist_ok=True)\n","\n","        if model and not self.args.no_save:\n","            # Take care of distributed/parallel training\n","            model_to_save = model.module if hasattr(model, \"module\") else model\n","            model_to_save.save_pretrained(output_dir)\n","            self.tokenizer.save_pretrained(output_dir)\n","            torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\n","            if optimizer and scheduler and self.args.save_optimizer_and_scheduler:\n","                torch.save(\n","                    optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\")\n","                )\n","                torch.save(\n","                    scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\")\n","                )\n","            self.save_model_args(output_dir)\n","\n","        if results:\n","            output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n","            with open(output_eval_file, \"w\") as writer:\n","                for key in sorted(results.keys()):\n","                    writer.write(\"{} = {}\\n\".format(key, str(results[key])))\n","\n","    def save_model_args(self, output_dir):\n","        os.makedirs(output_dir, exist_ok=True)\n","        self.args.save(output_dir)\n","\n","    def _load_model_args(self, input_dir):\n","        args = ClassificationArgs()\n","        args.load(input_dir)\n","        return args\n","\n","    def get_named_parameters(self):\n","        return [n for n, p in self.model.named_parameters()]\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ethyx9Y7_7Tf"},"source":["df = train\n","df['split'] = np.random.randn(df.shape[0], 1)\n","\n","msk = np.random.rand(len(df)) <= 0.7\n","\n","train = df[msk]\n","val2 = df[~msk]\n","train = train[['text','labels']]\n","val2 = val2[['text','labels']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xUEtqw7Srx1y"},"source":["model = ClassificationModelZ('bert', '/content/drive/MyDrive/modeller/output_bert_large/output_bert_base', num_labels=3,use_cuda=False, args={'learning_rate':1e-5, 'num_train_epochs': 10, 'reprocess_input_data': True,'save_model_every_epoch':False, 'overwrite_output_dir': True,\"scheduler\":\"constant_schedule\",\"optimizer\":\"AdamW\"})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xalAd2540Mt8"},"source":["!rm -r outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lZ3ACuWKXirz"},"source":["bert all"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SLBMzDN_XjxW","executionInfo":{"elapsed":15416,"status":"ok","timestamp":1635939731490,"user":{"displayName":"z erva erg√ºn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16255015835614953460"},"user_tz":-180},"outputId":"df12df7e-b02b-460b-e863-1123797db516"},"source":["import pandas as pd\n","import numpy as np\n","from simpletransformers.model import TransformerModel, ClassificationModel\n","from transformers import BertTokenizer\n","from torch.utils.data import TensorDataset\n","from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n","\n","from transformers import BertForSequenceClassification\n","#model = ClassificationModel(\"roberta\",\"/content/drive/MyDrive/modeller/output_roberta_base/output_roberta_base\", num_labels=3,use_cuda=False, args={'learning_rate':1e-5, 'num_train_epochs': 10, 'reprocess_input_data': True, 'overwrite_output_dir': True,'save_model_every_epoch': False,\"scheduler\":\"constant_schedule\"}) #)#('bert', '/content/drive/MyDrive/100mbout/1GB-bert'\n","#model = ClassificationModelZ('bert', '/content/drive/MyDrive/modeller/output_bert_base/output_bert_base', num_labels=3,use_cuda=False, args={'learning_rate':1e-5, 'num_train_epochs': 10, 'reprocess_input_data': True, 'overwrite_output_dir': True,\"scheduler\":\"constant_schedule\",\"optimizer\":\"AdamW\"})\n","#model.train_model(train)\n","model = ClassificationModelZ(\"bert\",\"/content/drive/MyDrive/modeller/output_bert_base/output_bert_base\", num_labels=3,use_cuda=False, args={'learning_rate':1e-5, 'num_train_epochs': 10, 'reprocess_input_data': True, 'overwrite_output_dir': True,'evaluate_during_training': False,'save_model_every_epoch': False,\"scheduler\":\"stlr\",\"optimizer\":\"all\"}) #)#('bert', '/content/drive/MyDrive/100mbout/1GB-bert'\n"],"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/drive/MyDrive/modeller/output_bert_large/output_bert_large were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/modeller/output_bert_large/output_bert_large and are newly initialized: ['classifier.weight', 'classifier.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","metadata":{"id":"S0l0XeMiX9k3"},"source":["from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n","\n","\n","def f1_multiclass(labels, preds):\n","    return f1_score(labels, preds, average='micro')\n","def f1_multiclass2(labels, preds):\n","    return f1_score(labels, preds, average='macro')\n","def f1_multiclass3(labels, preds):\n","    return f1_score(labels, preds, average='weighted')\n","def prec_score(labels, preds):\n","    return precision_score(labels, preds, average='micro')\n","def prec_score2(labels, preds):\n","    return precision_score(labels, preds, average='macro')\n","def prec_score3(labels, preds):\n","    return precision_score(labels, preds, average='weighted')\n","def recall(labels, preds):\n","    return recall_score(labels, preds, average='micro')\n","def recall2(labels, preds):\n","    return recall_score(labels, preds, average='macro')\n","def recall3(labels, preds):\n","    return recall_score(labels, preds, average='weighted')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NKUeN9FlmmeN"},"source":["# Working Site"]},{"cell_type":"code","metadata":{"id":"vLDNVZY-OiYI"},"source":["rm -r outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HMwffRPVWC2P"},"source":["manipulate the layering variable and observe the effects of layers"]},{"cell_type":"code","metadata":{"id":"GXCxqTooWAG_"},"source":["for i in range(12):\n","  print(\"Layer \",i,'\\n')\n","  model = ClassificationModelZ(\"bert\",\"/content/drive/MyDrive/modeller/output_bert_base\",i ,num_labels=3,use_cuda=False, args={'learning_rate':1e-5, 'num_train_epochs': 6, 'reprocess_input_data': True, 'overwrite_output_dir': True,'evaluate_during_training': False,'save_model_every_epoch': False}) #)#('bert', '/content/drive/MyDrive/100mbout/1GB-bert'\n","  model.train_model(train)\n","  result, model_outputs, wrong_predictions = model.eval_model(val,acc=accuracy_score, f1=f1_multiclass2, prec=prec_score2, recall = recall2)\n","  print(result)\n","  torch.save(model,\"/content/drive/MyDrive/layersofbert/layer\"+str(i))\n","  %rm -r outputs "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["when you **remove the layering variable in** the ClassificationModelZ and manipulate scheduler and optimizer variables in the \"args\" test the catastrophic forgetting effects"],"metadata":{"id":"WAGlQtUJnulL"}},{"cell_type":"code","source":["model = ClassificationModelZ('xlnet', '/content/drive/MyDrive/modeller/2005li/xlnet-large/xlnet-large/output-xlnet-large', num_labels=3,use_cuda=False, args={'learning_rate':1e-5, 'num_train_epochs': 15, 'reprocess_input_data': True, 'overwrite_output_dir': False,\"save_model_every_epoch\":False,\"scheduler\":\"constant_schedule\"})\n","model.train_model(train,val2)"],"metadata":{"id":"JGBEZ9fwnr6Z"},"execution_count":null,"outputs":[]}]}